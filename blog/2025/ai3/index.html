<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Poorly Structured Notes on AI Part 3 | Daniel Earwicker </title> <meta name="author" content="Daniel Earwicker"> <meta name="description" content="You are almost certainly lost on the Internet. "> <meta name="keywords" content="coding, physics"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon.png?754e6b58bcf99910aed4f8d8638bd47f"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://danielearwicker.github.io/blog/2025/ai3/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Daniel</span> Earwicker </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Poorly Structured Notes on AI Part 3</h1> <p class="post-meta"> Created on April 16, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/ai"> <i class="fa-solid fa-hashtag fa-sm"></i> ai</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>So what is the significance of the “attention” pattern? The three matrices have names: Query ($Q$), Key ($K$) and Value ($V$).</p> <p>Our inputs are vectors, $\vec{I}$, and we use the matrices as operators to do this:</p> \[A_{\mu\nu} = \hat{Q}\vec{I_{\mu}} \cdot \hat{K}\vec{I_{\nu}}\] <p>The dot product is at its positive maximum when the two vectors point in the same direction, at its negative minimum when they point in the exact opposite direction, and is zero when they are orthogonal. So we’re performing that comparison between every possible pairing of the (transformed) input vectors, to produce a square matrix the same size as the input token stream.</p> <p>Why perform a different transformation on the two sides of the dot product? This makes it asymmetrical. If the same matrix (say, $\hat{Q}$) was applied to both sides, $A$ would end up a symmetric matrix, nearly half its contents redundant. Using the whole matrix, breaking the symmetry, means that the effect of token $a$ on token $b$ is different from the effective of $b$ on $a$. The matrices $Q$ and $K$ twist and flip each input vector differently.</p> <p>And then (after $\text{softmax}$ fixes it up) $A$ goes to work on the vectors $\hat{V} \vec{I}_{\nu}$, that is, the input vectors again flipped around by another matrix.</p> <p>Can we visualise this as a network with information flowing? It’s kind of a struggle. First, it’s a network that springs into existence temporarily, rather than a persistent structure in the model. Each token of the input is a node, and so the number of nodes depends on the length of the input. And what are the edges, and what travels along them?</p> <p>The names of the matrices hint at their specific purposes: $Q$(uery) is asking a question. e.g.</p> <blockquote> <p>The query ($Q$) can be thought of as a representation of the current task at hand (e.g., “What word follows too?”).</p> <p>— Foster, David. <em>Generative Deep Learning</em></p> </blockquote> <p>Whereas $K$(ey) is like an identifier in a mapping from keys to values:</p> <blockquote> <p>The key vectors ($K$) are representations of each word in the sentence—you can think of these as descriptions of the kinds of prediction tasks that each word can help with. They are derived in a similar fashion to the query…</p> <p>— <em>ibid</em>.</p> </blockquote> <p>But not just in a similar fashion - in an identical fashion. It’s up to the model, in whatever happens during the training process, to distribute information between these matrices, and they work identically, so it seems a little fanciful to dress them up with these specific roles (however vaguely defined).</p> <p>Really I think all we can say is that by pre-transforming the input vectors by two different operators, the symmetry of the dot product is deliberately broken and the resulting $A$ matrix is enriched with as much information as possible, by not being a symmetric matrix.</p> <blockquote> <p>The value vectors ($V$) are also representations of the words in the sentence — you can think of these as the unweighted contributions of each word.</p> <p>– <em>ibid</em>.</p> </blockquote> <p>In a traditional fully-connected layer, we can think of it having a plastic network structure such that as it is trained, the strengths of the connections between nodes is gradually modified, some may weaken to almost nothing, and so there is eventually a specific persistent network that holds knowledge. We can picture it topologically.</p> <p>But here, the network (if it’s appropriate to think of it in that way at all) is an ephemeral thing that springs into existence, the number of nodes depending on the number of tokens in the input, their influence on each other being described by $A$, which is custom-generated to suit this length of input. It’s very different from an old-school artificial neural network, and very many steps removed from anything in the brain.</p> <p>And we’re not even part of the way there yet!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Not yet regretting the time you've spent here?</h2> <p class="mb-2">Keep reading:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/ai2/">Poorly Structured Notes on AI Part 2</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/ai1/">Poorly Structured Notes on AI Part 1</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/Prompt-Engineer/">How to become a prompt engineer</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/Time-reversible-events/">Time reversible events</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/Java-Csharp/">Language Smackdown: Java vs. C#</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Daniel Earwicker. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>