<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://danielearwicker.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://danielearwicker.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-04-09T22:39:36+00:00</updated><id>https://danielearwicker.github.io/feed.xml</id><title type="html">blank</title><subtitle>You are almost certainly lost on the Internet. </subtitle><entry><title type="html">Poorly Structured Notes on AI Part 2</title><link href="https://danielearwicker.github.io/blog/2025/ai2/" rel="alternate" type="text/html" title="Poorly Structured Notes on AI Part 2"/><published>2025-04-08T00:00:00+00:00</published><updated>2025-04-08T00:00:00+00:00</updated><id>https://danielearwicker.github.io/blog/2025/ai2</id><content type="html" xml:base="https://danielearwicker.github.io/blog/2025/ai2/"><![CDATA[<p>The input vectors \(\vec{I}_{n}\) to the transformer are therefore vectors consisting of $D_{model}$ numbers, found by vector addition of the embedding vectors \(\vec{E}_n\) for each token to the position vector \(\vec{P}_n\) for the token’s position $n$ in the input stream:</p> \[\vec{I}_n = \vec{E}_n + \vec{P}_n\] <p>The \(\vec{P}_n\) only depends on $n$, and requires that \(D_{model}\) is an even number because it populates the dimensions in pairs, each pair taking on the coordinates of a point tracing out a circle at a different frequency.</p> <p>Transformer implementations vary in their details. They may have an <em>encoder</em>, or a <em>decoder</em>, or both. The purpose of these components is not obvious until you’ve seen how they work independently or together in actual applications.</p> <p>We’ll start with the encoder. The $\vec{I}_n$ inputs are fed into the first of several layers. The layers are structurally identical black boxes, but maintain their own parameters. As the model is trained, knowledge is distributed across all the layers.</p> <p>A layer has three matrices, $Q_{ij}$, $K_{ij}$ and $V_{ij}$. To simplify, we’ll assume they are square $D_{model} \times D_{model}$. In real implementations this part is done in parallel by multiple “heads”, each working on a slice of the dimensions, so these matrices will be $D_{model} \times (D_{model}/h)$ where $h$ is the number of heads, and each head has its own triplet of matrices. We can note that this is possible and then forget about it as it’s an optimisation; $h$ can be set to $1$.</p> <p>The three matrices are used as operators to transform each input vector into a new vector which we’ll name after the matrix but in lowercase:</p> \[\vec{q} = Q \vec{I}\] \[\vec{k} = K \vec{I}\] \[\vec{v} = V \vec{I}\] <p>If you prefer abstract tensor notation you can think of the input as a matrix $I_{nd}$ with a row for each token and a column for each embedding vector dimension, so to get the $d$-th coordinate of the $n$-th projected $\vec{q}$ vector:</p> \[q_{nd} = Q_{kd} I_{nk}\] <p>The nice thing about that notation is that it literally tells you (with no need to remember how matrix multiplication works!) which ordinary numbers to multiply, summing over the values of $k$ in the range $D_{model}$, thus computing one element of a matrix $q_{nd}$ at row $n$ and column $d$, which we can also think of as a list of $n$ vectors $\vec{q}_n$.</p> <p>Next the layer computes the attention weights $A_{\alpha\beta}$, where those indices $\alpha, \beta$ range over the number of tokens in the input. So this is a variable size square matrix relating every token to every other token in the input. Therefore it is immediately clear that it isn’t stored anywhere (the model has a fixed number of parameters, so it can’t depend on the length of a given stream of input.) It’s a “temporary variable” of this model iteration, used once and discarded.</p> <p>The first step to computing these weightings is to get scores $\sigma_{\alpha\beta}$:</p> \[\sigma_{\alpha\beta} = q_{\alpha\gamma} k_{\gamma\beta}\] <p>Or if you prefer, generate a list of vectors of the same length as the input token stream, the vector dimensions also being length of the input token stream, by taking the dot product of each $\vec{q}$ with every $\vec{k}$.</p> <p>Now, because of what we’re going to do next, we want to keep the scores in a small range, so their variance doesn’t depend on the models chosen $D_{model}$. Intuitively, suppose you have a set of numbers that are all either $1$ or $-1$ randomly (like coin flips). If you sum them, that’s like taking a random walk up and down the number line, starting at $0$. It’s a famous result that your expected distance from the origin will be $\sqrt{N}$ after $N$ steps. What we’re saying here is that we want the distance to be $1$, regardless of the number of steps, and the dot product requires us to add $D_{model}$ numbers, so we need to divide the score values by $\sqrt{D_{model}}$.</p> <p>And now we are ready to use the <code class="language-plaintext highlighter-rouge">softmax</code> function. This is widely used in machine learning and is sometimes described as producing a “probability distribution” but it does nothing of the sort. Given a vector, it returns another vector whose coordinates are all in the interval $(0, 1)$ and sum to $1$, so they certainly look as if they could be a probability distribution. Note that this is not the same as normalising the vector (making the squares of its components sum to $1$.)</p> <p>It does this by exponentiating each coordinate and dividing it by the the sum of all the exponentiated coordinates. Exponentiation has the effect of blowing up large values and shrinking small values, so it tends to single out and exaggerate a few spiky coordinates while suppressing others. This is one reason for normalising the variance of the raw scores, to lessen this effect. (Note that <code class="language-plaintext highlighter-rouge">softmax</code> is applied separately to each scaled score vector, so the coordinates sum to $1$ within one vector, not over all coordinates of all vectors.)</p> <p>And so by applying <code class="language-plaintext highlighter-rouge">softmax</code> on each vector, we finally have our attention weights, and we can use them to transform the $\vec{v}$ vectors, which are the only ones we still haven’t made use of:</p> \[o_{nd} = A_{n\lambda} v_{\lambda d}\] <p>Once again, think of $\lambda$ being the summation variable iterating over the length of the input token stream.</p> <p>This concludes the “attention” part of what the layer does, but we’re not done yet. We haven’t introduced nearly enough parameters to support NVIDIA’s share price. But it’s bedtime again.</p>]]></content><author><name></name></author><category term="ai"/><summary type="html"><![CDATA[The input vectors \(\vec{I}_{n}\) to the transformer are therefore vectors consisting of $D_{model}$ numbers, found by vector addition of the embedding vectors \(\vec{E}_n\) for each token to the position vector \(\vec{P}_n\) for the token’s position $n$ in the input stream:]]></summary></entry><entry><title type="html">Poorly Structured Notes on AI Part 1</title><link href="https://danielearwicker.github.io/blog/2025/ai1/" rel="alternate" type="text/html" title="Poorly Structured Notes on AI Part 1"/><published>2025-04-07T00:00:00+00:00</published><updated>2025-04-07T00:00:00+00:00</updated><id>https://danielearwicker.github.io/blog/2025/ai1</id><content type="html" xml:base="https://danielearwicker.github.io/blog/2025/ai1/"><![CDATA[<p>An AI model has a lot of parameters organised into matrices. These could be initially assigned random values. The training process will adjust them so they ultimately represent knowledge in some sense.</p> <p>Considering the example of handling written words, the first step is to associate every word in the vocabulary with a unique integer. So we just add every new word we encounter to a big ol’ list, so their position in the list is their unique number. Note that in practice we might not work at the level of words; more generally we talk about <em>tokens</em>, which could be parts of words or groups of adjacent words, so we’ll switch to saying tokens from now on. But feel free to picture them as whole words from the dictionary.</p> <p>We build our massive table of all tokens by tokenizing our entire training data set (say, Wikipedia and some copyrighted books we illegally downloaded.) So now we have a list of $T$ known tokens, all numbered, and we can convert any text into a sequence of numbers.</p> <p>Then we get the first hint of the curious importance of geometry in this discipline. We maintain a vector associated with each token with ever encountered. Each vector is represented by an array of numbers. How long is the array? A fixed length that is a permanent characteristic of the model, called $D_{model}$ - $D$ is for dimensions, as this the number of dimensions of one of these vectors, which we will find is an even number.</p> <p>But as we have one such vector for every known token, this can be seen as our first matrix: think of it having $D_{model}$ columns and $T$ rows. All the numbers in all the slots are assigned a random value initially. Admittedly so far nothing particularly geometrical has happened, except that we called this a list of vectors.</p> <p>So given some new incoming text that we’ve tokenized and turned into a sequence of integer token identifiers, we can fetch the vector representation associated with each token in the sequence, and now we have a sequence of vectors. These are called <em>embedding</em> vectors.</p> <p>Important: do not try to understand the meaning of the numbers in these vectors in isolation. They start off random, and they are going to be adjusted during training in a way that depends on <em>everything else about this model</em>. You can understand the abstract machinery of how this works by considering each piece on its own, but you can’t get even a hint of why it might possibly work without considering the whole thing end to end, and we’re not there yet.</p> <p>The next thing we’re going to do is mix in positional information. The approach may seem reminiscent of Fourier analysis or AM radio. Think of a clock’s second hand ticking round and round. It has a certain frequency at which it makes one journey round the circle: one cycle per minute. The clock hand is a vector. We can describe in it coordinates using:</p> \[x = \cos 2\pi t / 60\] \[y = \sin 2\pi t / 60\] <p>The scaling applied to the time $t$ determines how fast it cycles. Now think of time ticking forward in integer increments, so $t$ is an integer.</p> <p>We could have a whole line of clocks of different frequencies, getting slower along the line. One formula for the $i$-th clock is:</p> \[x = \cos \frac{t}{b^{i/i}}\] \[y = \sin \frac{t}{b^{i/I}}\] <p>The term on the bottom of the fraction is some base number $b$ raised to the power $i/I$, where $i$ is the position of the clock in the line-up and $I$ is the number of clocks (say, 500). Supposing we number the first clock as $i=1$, anything to the power $1/500$ is a little larger than $1$, so the value of $t$ is passed almost unmodified to $\sin$ and $\cos$. By the time we get to $i=I$ the fraction $i/I$ is $1$ so we’re passing $t/b$ to $\sin$ and $\cos$. There’s a smooth curve in between, but clocks are distinct, discrete objects so we sample the curve at integer values of $i$.</p> <p>Why do we need two functions? $\sin$ and $\cos$ are the same function phase-shifted by a quarter cycle. What is gained by both of them? The instantaneous value of $\sin$ alone passes through zero twice per cycle, once on the upswing and once on the downswing. But by also evaluating $\cos$ it is possible to distinguish the upswing and downswing of $\sin$ and vice versa, and know where you are in the cycle from a snapshot. There is something deeply significant about how you need the full picture of the circle, not just one of the coordinates oscillating.</p> <ul> <li>Physical example: a pendulum (which is approximately a harmonic oscillator at small angles) may have position described by $\sin$, but to fully know its state at some instant you also need to know its momentum, which will be proportional to $\cos$, always one quarter cycle out of phase with the position. So its actual state traces out an ellipse in “phase space”.</li> <li>Pure mathematics: $e^{i\theta}$ traces out the unit circle in the complex plane, being equal to $\cos \theta + i \sin \theta$.</li> </ul> <p>Anyway, what does this have to do with positional encoding? Replace the passing of time $t$ with the zero-based row number $p$ in a matrix, which we’ll call the PE matrix, and let the number of rows equal the number of tokens in the current input. There are $D_{model}$ columns that we number with $i$. We fill in the matrix values by alternating between the $\sin$ and $\cos$ formulae. So each pair of adjacent columns describes a clock handle that is circling in discrete jumps as we scan down the rows. (You could also think of their being half as many columns, but each column holds complex values, and each row is generated by multiplying the previous row’s value by some constant multiple of the imaginary number $i$, the constant shrinking as we look along the columns.)</p> <p>We can generate this matrix for any number of rows, and it’s <em>always the same</em>, it’s not something that will be adjusted by training.</p> <p>The key to positional encoding is that for the input token at position $p$, we add the row $p$ of the positional encoding matrix to the embedding vector (remember them?) of the token $p$. The resulting set of vectors is passed to the next stage.</p> <p>Where similar tokens appear in two places in the input, one of the pairs of dimensions will encode this similarity because its cycle period will happen to align with the distance between the two tokens. This has something of the flavour of AM radio, where there are many carrier frequencies modulated by actual information signals, or Fourier analysis. But there are some major differences:</p> <ul> <li>here we’re summing the vectors rather than multiplying amplitudes,</li> <li>we aren’t super-positioning (adding) all the frequencies, we’re keeping them separate in their own (pairs of) columns,</li> <li>the information signal actually starts off random, and we’re going train it to hold real information.</li> </ul> <p>Now we finally have the input vectors to the transformer, but it’s time for a break.</p>]]></content><author><name></name></author><category term="ai"/><summary type="html"><![CDATA[An AI model has a lot of parameters organised into matrices. These could be initially assigned random values. The training process will adjust them so they ultimately represent knowledge in some sense.]]></summary></entry><entry><title type="html">How to become a prompt engineer</title><link href="https://danielearwicker.github.io/blog/2024/Prompt-Engineer/" rel="alternate" type="text/html" title="How to become a prompt engineer"/><published>2024-04-24T00:00:00+00:00</published><updated>2024-04-24T00:00:00+00:00</updated><id>https://danielearwicker.github.io/blog/2024/Prompt-Engineer</id><content type="html" xml:base="https://danielearwicker.github.io/blog/2024/Prompt-Engineer/"><![CDATA[<p>Added a run-through of <a href="talks/">a talk I gave today</a> about an application of LLMs I worked on in mid-2023.</p>]]></content><author><name></name></author><category term="ai"/><category term="talks"/><summary type="html"><![CDATA[Added a run-through of a talk I gave today about an application of LLMs I worked on in mid-2023.]]></summary></entry><entry><title type="html">Time reversible events</title><link href="https://danielearwicker.github.io/blog/2023/Time-reversible-events/" rel="alternate" type="text/html" title="Time reversible events"/><published>2023-04-07T00:00:00+00:00</published><updated>2023-04-07T00:00:00+00:00</updated><id>https://danielearwicker.github.io/blog/2023/Time-reversible-events</id><content type="html" xml:base="https://danielearwicker.github.io/blog/2023/Time-reversible-events/"><![CDATA[<p>The current state of a system might be represented by the contents of a database table. The table could have many columns of various data types, but to simplify we’ll say there is only a single integer column, so our table is just a list of integers. (Each integer could in reality be a foreign key into another table holding immutable and distinct tuples, each describing a frozen configuration of a more interesting entity such as a person, a medical record and so on. So we can make this simplification without loss of generality.)</p> <p>We can describe the history of the table’s state by creating another table in which the rows represent <em>events</em>. Of course in a database we need to think about atomic transactions; some kinds of change may not make sense by themselves and must always occur atomically as part of a transaction along with certain related changes. Therefore an event always belongs to a <em>batch</em>, and a batch may contain multiple events. Batches occur in a definite order, so we can number them (that is, the primary key of a batch is a sequence number). A batch is also the ideal place to record the clock time that the events in the batch were applied.</p> <p>Aside from the <code class="language-plaintext highlighter-rouge">batch_id</code> an event has just two columns:</p> <ul> <li>the <code class="language-plaintext highlighter-rouge">value</code> that would appear in the current state table</li> <li>a bit <code class="language-plaintext highlighter-rouge">added</code> that is 1 (true) if the value was added or 0 (false) if it was removed (the only other possible event in such a simple system)</li> </ul> <p>Given this table, we can begin with an empty state table, and “play back” the changes, inserting the <code class="language-plaintext highlighter-rouge">value</code> if <code class="language-plaintext highlighter-rouge">added</code> is 1 and removing it if <code class="language-plaintext highlighter-rouge">added</code> is 0, and we will reconstruct the latest state.</p> <p>When deleting, we remove one instance of the specified <code class="language-plaintext highlighter-rouge">value</code>; any two instances of the same <code class="language-plaintext highlighter-rouge">value</code> are equivalent so it doesn’t matter which we delete. All that matters is how many instances there are. Generally is possible that the events in a batch might arrive at a net effect by some redundant route:</p> <table> <thead> <tr> <th style="text-align: center"><code class="language-plaintext highlighter-rouge">value</code></th> <th style="text-align: center"><code class="language-plaintext highlighter-rouge">added</code></th> </tr> </thead> <tbody> <tr> <td style="text-align: center">3</td> <td style="text-align: center">1</td> </tr> <tr> <td style="text-align: center">3</td> <td style="text-align: center">0</td> </tr> <tr> <td style="text-align: center">3</td> <td style="text-align: center">0</td> </tr> <tr> <td style="text-align: center">3</td> <td style="text-align: center">1</td> </tr> <tr> <td style="text-align: center">3</td> <td style="text-align: center">1</td> </tr> </tbody> </table> <p>This has the net effect of adding 3, as there is one more added than removed. The only danger with such a situation is that order in which the events are replayed is important, as if all the removes were performed first, this could imply that <code class="language-plaintext highlighter-rouge">3</code> occurs a negative number of times in the state table! But such a batch could be simplified at the point of its creation to remove both the unnecessary events and also this problem.</p> <p>To be certain that our events describe the complete history, it would be safest to never directly update the state table, but instead write a batch of events and then execute that batch as described above. The event table is the definitive record of what has occurred. The latest state, arrived at by executing the events, is merely a convenient representation <em>derived</em> from the events.</p> <p>And if we are mostly interested in recent past states, we would prefer to start with the latest state and “rewind” back to a recent past state, if only because this is a shorter journey than starting from the ever-more-distant empty initial state and playing forward. Fortunately our events have an interesting property: we can flip the <code class="language-plaintext highlighter-rouge">added</code> bit, setting it to <code class="language-plaintext highlighter-rouge">1 - added</code>. This makes each “add” into a “remove” and vice versa.</p> <p>This backward time travel can be implemented easily by making our algorithm for playing back events accept an <code class="language-plaintext highlighter-rouge">undo</code> bit parameter. An event causes an insertion if <code class="language-plaintext highlighter-rouge">added != undo</code>, and it causes a removal if <code class="language-plaintext highlighter-rouge">added == undo</code>. Think it through:</p> <ul> <li>playing forwards: <code class="language-plaintext highlighter-rouge">undo</code> is 0, and so <code class="language-plaintext highlighter-rouge">added != 0</code> implies adding, while <code class="language-plaintext highlighter-rouge">added == 0</code> implies removal, as we originally said.</li> <li>playing backwards: <code class="language-plaintext highlighter-rouge">undo</code> is 1, and so <code class="language-plaintext highlighter-rouge">added != 1</code> implies adding, while <code class="language-plaintext highlighter-rouge">added == 1</code> implies removal, so <code class="language-plaintext highlighter-rouge">added</code> has the opposite meaning.</li> </ul> <p>There’s an amusing similarity here with quantum field theory, in which a positron can be thought of as an electron moving backward through time.</p> <p>What makes an event stream time-reversible? There must be no loss of information. In databases we often avoid immutable approaches, and instead overwrite values in existing records. To support this we nominate one column to be immutable so it can act as the primary key. An event that says:</p> <blockquote> <p>In row 15, change <code class="language-plaintext highlighter-rouge">last_name</code> to <code class="language-plaintext highlighter-rouge">"Smith"</code></p> </blockquote> <p>is <em>destructive</em>, as the previous value is lost. The destruction of information rules out reversibility. But capturing it as:</p> <blockquote> <p>In row 15, change <code class="language-plaintext highlighter-rouge">last_name</code> from <code class="language-plaintext highlighter-rouge">"Jones"</code> to <code class="language-plaintext highlighter-rouge">"Smith"</code></p> </blockquote> <p>is enough to restore reversibility. To convert to an undo operation, we simply swap the values in the <em>from</em> and <em>to</em> slots. No doubt we will also support a delete event, in which case the event will have to capture its entire contents:</p> <blockquote> <p>Delete row 15, in which <code class="language-plaintext highlighter-rouge">last_name</code> is <code class="language-plaintext highlighter-rouge">"Smith"</code>, <code class="language-plaintext highlighter-rouge">first_name</code> is…</p> </blockquote> <p>When that event is reversed, it becomes:</p> <blockquote> <p>Insert row 15, in which <code class="language-plaintext highlighter-rouge">last_name</code> is <code class="language-plaintext highlighter-rouge">"Smith"</code>, <code class="language-plaintext highlighter-rouge">first_name</code> is…</p> </blockquote> <p>(Note that the primary key 15 has to be retained, as there must be at least one other event in the past referring to row 15, so we can’t use the popular RDBMS feature that creates a new key from a counter on each insert.)</p> <p>The rules for how to time-reverse an event may appear haphazard, but really they are perfectly consistent. The “change” event is really two events, one that removes a value and the other that adds, so in every case we are in fact turning add into remove and vice versa.</p> <p>Even so, in the simpler scheme where our current state is just integers (which are foreign keys into another table holding immutable records, retaining every distinct combination of values we’ve ever encountered), this quite involved event vocabulary is unnecessary.</p>]]></content><author><name></name></author><category term="events"/><summary type="html"><![CDATA[The current state of a system might be represented by the contents of a database table. The table could have many columns of various data types, but to simplify we’ll say there is only a single integer column, so our table is just a list of integers. (Each integer could in reality be a foreign key into another table holding immutable and distinct tuples, each describing a frozen configuration of a more interesting entity such as a person, a medical record and so on. So we can make this simplification without loss of generality.)]]></summary></entry><entry><title type="html">Language Smackdown: Java vs. C#</title><link href="https://danielearwicker.github.io/blog/2023/Java-Csharp/" rel="alternate" type="text/html" title="Language Smackdown: Java vs. C#"/><published>2023-03-07T00:00:00+00:00</published><updated>2023-03-07T00:00:00+00:00</updated><id>https://danielearwicker.github.io/blog/2023/Java-Csharp</id><content type="html" xml:base="https://danielearwicker.github.io/blog/2023/Java-Csharp/"><![CDATA[<p>A pithy quote:</p> <blockquote> <p>There are only two kinds of languages: the ones people complain about and the ones nobody uses.</p> </blockquote> <p>Now you might say that’s exactly what the creator of C++ would say to cover his tracks. But the point is that Java and C# are languages that are 20 to 25 years old, widely used (maybe 15 million users between them), and are both cursed with toxic corporate associations. When Java first came along it was <em>cool</em>, if a programming language ever could be. But this was because the only Java code in the wild was neat little animations and things like that. As soon as it became widely used for boring line-of-business apps, it began to be thought of as the new COBOL.</p> <p>But the corporate toxicity is the clincher. Microsoft, it is generally believed, only created C# because they couldn’t find a way to take over Java without getting into further legal difficulties. Meanwhile Oracle is perhaps the only corporation that could rival Microsoft for historical unpopularity, and yet they found a way to take over Java, and happily continue creating legal difficulties for others.</p> <p>But despite all this, Java and C# are worth studying, because they started off deliberately similar, and have diverged, and then somewhat reconverged, so we can see the same ideas being twisted in different directions. Two sort-of similar languages that have evolved separately are like different universes in the multiverse, and that’s interesting.</p> <p>Also, the way these languages are judged is quite irrational. Look at C, which remains hugely popular, widely used, a terrible choice for most applications, and ultimately originates from a corporation (AT&amp;T) that wasn’t allowed to sell it as a product as they were bound by an antitrust settlement.</p> <h1 id="two-households-both-alike-in-dignity">Two households, both alike in dignity</h1> <p>Even from the very beginning there were certain key differences that had long term consequences. C# had the advantage of being able to learn from Java, such that many of the <a href="http://www.javapuzzlers.com">odd edge cases and pitfalls</a> that can occur in the earlier language are automatically ruled out.</p> <p>Take for example Java’s schism between primitives (<code class="language-plaintext highlighter-rouge">boolean</code>, <code class="language-plaintext highlighter-rouge">char</code>, <code class="language-plaintext highlighter-rouge">int</code>, <code class="language-plaintext highlighter-rouge">long</code>, <code class="language-plaintext highlighter-rouge">double</code>…) and everything else. Primitives fit into a little space so that’s all they ever need to be: a patch of memory intepreted as a number. Whereas everything else, and that includes things as simple as a <code class="language-plaintext highlighter-rouge">String</code> or a nice <code class="language-plaintext highlighter-rouge">ComplexNumber</code> class, is far more complicated. A <code class="language-plaintext highlighter-rouge">String</code> variable holds the address of an object, and that object has a header that takes up a bunch of space, and multiple variables may point to the same object, which implies a decision has to be taken over when it’s safe to throw away the object, so it has to be managed by GC. To be sure, all this stuff sounds worse than it is, and most of the time in most applications it doesn’t matter a bit (which is why Java has millions of users despite all this).</p> <p>But consider that <code class="language-plaintext highlighter-rouge">ComplexNumber</code>. Just to be extra unfair, let’s say that on a 64-bit platform we’re using two 32-bit <code class="language-plaintext highlighter-rouge">float</code>s (total 8 bytes), to which the Java runtime adds a standard header big enough to hold two pointers, so that is another 16 bytes. If you’re storing a lot of these objects in memory, you might care that this has tripled your memory requirement. And you’re probably holding them all in an array, or should I say “in” an array: the array doesn’t store the objects, just their addresses, which means there’s another 8 extra bytes per object. So 8 has become 32, and 75% of your memory is being used to store bureaucratic paperwork.</p> <p>But C# has your back, because you can define your own primitives with the <code class="language-plaintext highlighter-rouge">struct</code> keyword and know that they are packed tightly into memory just like individual <code class="language-plaintext highlighter-rouge">float</code>s are. There’s still a schism, because your type must be either <code class="language-plaintext highlighter-rouge">struct</code> or <code class="language-plaintext highlighter-rouge">class</code>. But the difference is somewhat papered over because at the root of the type system, “everything’s an object”. If you have a variable of type <code class="language-plaintext highlighter-rouge">object</code>, you can assign an instance of your <code class="language-plaintext highlighter-rouge">ComplexNumber</code> to it, through the miracle-kludge of automatic boxing: the language has the ability to treat a <code class="language-plaintext highlighter-rouge">struct</code> as if it were a <code class="language-plaintext highlighter-rouge">class</code>, where required (though not vice versa).</p> <p>These things are likely to gradually make their way into Java over the next few years though:</p> <ul> <li><a href="https://openjdk.java.net/jeps/401">JEP 401: Primitive Classes</a></li> </ul> <h1 id="episodet---generics"><code class="language-plaintext highlighter-rouge">Episode&lt;T&gt;</code> - Generics</h1> <p>If you’re thinking of designing the next great general purpose language with static typing, please remember to include generics in the first version and save yourself a world of pain. You <em>are</em> going to add them, <a href="https://www.infoworld.com/article/3645228/go-118-arrives-with-much-anticipated-generics.html">even if you wait ten years</a>.</p> <p>In due course, Java (2004) and C# (2005) got them, in a near simultaneous development. And yet they ways they went about it almost couldn’t be more different. There’s a <a href="https://openjdk.java.net/projects/valhalla/design-notes/in-defense-of-erasure">great write-up of the Java approach here</a> but it’s notably quite a defensive account, and it has to be, because there’s a perception that, comparatively, C# “did it right”. Java aimed to make it so that a class author could add <code class="language-plaintext highlighter-rouge">&lt;T&gt;</code> to the end of the class’s name and still have the compiler produce a <code class="language-plaintext highlighter-rouge">.class</code> file that could be dynamically loaded by an application that was built before generics existed. This would allow generics to smoothly make their way into regular usage without breaking any applications.</p> <p>But it imposed some fairly extreme limitations that persist today. You cannot use a primitive as a type argument, and you cannot find out anything much about generic types at runtime. Java introduced auto-boxing for primitives along similar lines to C#, but as noted above, this means that if you need to conserve memory when dealing with large quantities of primitive values, then you can forget about generics. C# avoids that problem entirely: you can say <code class="language-plaintext highlighter-rouge">MyClass&lt;int&gt;</code> and it will be like <code class="language-plaintext highlighter-rouge">int</code> is substituted everywhere that <code class="language-plaintext highlighter-rouge">T</code> appeared in the source. For value types, it’s very reminiscent of C++ templates, but only in the good aspects.</p> <p>As Brian Goetz tells it:</p> <blockquote> <p>C# made the opposite choice — to update their VM, and invalidate their existing libraries and all the user code that dependend [sic] on it. They could do this at the time because there was comparatively little C# code in the world; Java didn’t have this option at the time.</p> </blockquote> <p>But this isn’t really accurate. The get-out of entirely ignoring backward compatibility is a straw man, available to neither language. Certainly C# wasn’t as well established as Java by 2005, but it was on a very fast upward curve, and this meant it was even less able to afford to alienate its early adopters. The result - at best - would have been a Python 2 versus 3 scenario, which clearly didn’t happen, or a fatal crisis of confidence among users, which didn’t happen either. To see why, you can do a little experiment (I actually did this):</p> <ul> <li>Set up a C# 1.0 environment (I created, with some difficulty, a VM running Windows XP Service Pack 3 and installed Visual Studio .NET 2002.)</li> <li>Write a class library with some static methods that accept/return <code class="language-plaintext highlighter-rouge">ArrayList</code> and <code class="language-plaintext highlighter-rouge">Hashtable</code> objects.</li> <li>Build it and then copy the resulting DLL to your present day environment where you have .NET 6.</li> <li>Run <code class="language-plaintext highlighter-rouge">dotnet new console</code> to create an app and make it depend on your ancient relic DLL: call the methods, print the return values etc.</li> </ul> <p>You’ll find that it loads and runs just fine. An assembly <code class="language-plaintext highlighter-rouge">.dll</code> produced by the very first version of the C# compiler is still supported <em>today</em>. It even works on Linux, or Mac OS on Apple silicon. All the non-generic collection classes that were in version 1 still exist in today’s platform. So we can see that C# certainly didn’t make the decision to invalidate their (or anyone else’s) existing libraries or break binary compatibility at all. They made it possible for generic and non-generic classes to coexist side by side.</p> <p>So what is the actual difference in Java’s approach? If you use today’s C# tools to build a class that uses generic collections and then send it back to your simulation of 2002, that old version of the runtime won’t know what to do with it. The question is, did that ever really matter?</p> <p>I’d say that Java’s erasure approach placed the past on a pedestal, at the expense of the future. Generics are forever doomed to act like they don’t exist, to avoid upsetting the code written during Java’s first decade, and to the detriment of developers (and language designers) working in the subsequent two decades and beyond. The C# reification approach works like a compromise that doesn’t materially harm either side of the equation: from the perspective of the future it looks very well designed and complete, and yet to the past it was sufficiently accommodating, if not perfectly so.</p> <p>In C# objects genuinely know what type they are at runtime, and the reflection system provides a rich, explorable network of facts about how the types were generated. In Java, the reflection system is curiously stuck in the now distant past (well, almost; confusingly there are places in the reflection system that know about generics, but they are mostly stymied by erasure).</p> <p>The irony is that after all that care taken by Java to appease the past, it eventually did have to adopt <a href="https://www.theregister.com/2019/03/07/java_developers_version_8/">a more aggressive attitude to upgrading itself</a>, requiring users to get accustomed to having to upgrade their package dependencies to some newer compatible combination <a href="https://carlmastrangelo.com/blog/the-impossible-java-11">whenever they migrated to a new major version of the platform</a>. This isn’t actually as traumatic as it sounds, even though it does often involve code changes, and is the norm in most ecosystems.</p> <p>What’s perhaps a little worse is that because the use of erasure was so well known, and could therefore be depended on in code, it became commonplace for code to assume that a type parameter was always a reference type, based on <code class="language-plaintext highlighter-rouge">Object</code>, and therefore that it would be safe to allocate an <code class="language-plaintext highlighter-rouge">Object[]</code> and use it to store elements of a parameter type. This gets in the way of attempts to enhance the JVM to support allowing primitive type arguments in an efficient way. Also methods like <code class="language-plaintext highlighter-rouge">remove(Object item)</code> often retain that pre-2004 signature even for collections of <code class="language-plaintext highlighter-rouge">&lt;T&gt;</code>, which is pretty unhelpful; while it may not cause a type error at runtime, it can conceal a logical bug.</p> <p>Again, there is work in the pipeline:</p> <ul> <li><a href="https://openjdk.java.net/jeps/8261529">JEP draft: Universal Generics</a></li> </ul> <p>But it’s focused purely on avoiding the performance bottleneck imposed by only supporting reference types; there’s no plan to fully reify generics. There may be no way to do that now without creating a Python 2/3 style bifurcation.</p> <h1 id="iterating-neednt-be-irritating">Iterating needn’t be irritating</h1> <p>C# 1 included a neat feature: <code class="language-plaintext highlighter-rouge">foreach</code>. The equivalent appeared in Java 5, the same release that added generics. Strictly speaking it’s only a near equivalent. An important part of the protocol for such a loop is dealing with the case where the user breaks out early:</p> <div class="language-java highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="o">(</span><span class="kt">var</span> <span class="n">joke</span> <span class="o">:</span> <span class="n">jokeBook</span><span class="o">)</span> <span class="o">{</span>
    <span class="k">if</span> <span class="o">(</span><span class="n">inBadMood</span><span class="o">())</span> <span class="o">{</span>
        <span class="k">break</span><span class="o">;</span>
    <span class="o">}</span>
<span class="o">}</span>
</code></pre></div></div> <p>In the equivalent statements in C# (and JavaScript for that matter), when we leave the loop there is a way to notify the iterator so it can dispose of any resources it might be using. The <code class="language-plaintext highlighter-rouge">IEnumerator&lt;T&gt;</code> interface derives from <code class="language-plaintext highlighter-rouge">IDisposable</code>, which introduces the <code class="language-plaintext highlighter-rouge">Dispose</code> method.</p> <p>But this part of the iteration pattern is absent from Java. It was only added to C# in version 2, and perhaps not coincidentally this was at the same time C# gained iterator methods via <code class="language-plaintext highlighter-rouge">yield return</code>, another (truly awesome) feature that has an exact analog in JS and Python but not in Java.</p> <h1 id="java-snoozes-and-loses">Java snoozes and loses</h1> <p>There seems to have been some kind of politically-imposed stagnation that stopped Java making any significant enhancements between 2005-2011, possibly due to a sense that if generics were anything to go by, “new” means “bad”. Meanwhile during this period alone, C# gained, in addition to generics:</p> <ul> <li><code class="language-plaintext highlighter-rouge">yield return</code></li> <li>extension methods</li> <li>lambdas</li> <li><code class="language-plaintext highlighter-rouge">var</code> (inference of local variable types)</li> <li>anonymous types</li> <li>query expressions</li> <li>expression trees</li> <li>auto properties</li> <li><code class="language-plaintext highlighter-rouge">dynamic</code></li> <li>out/in on type parameters</li> <li>optional parameters and named arguments</li> </ul> <p>This period fundamentally changed what idiomatic C# code looked like, so that it gained entirely its own style that couldn’t be replicated in Java.</p> <h1 id="the-java-renaissance">The Java renaissance?</h1> <p>In 2011 some significant new features finally snuck in to Java, the most notable being the <code class="language-plaintext highlighter-rouge">try</code>-with-resources statement, because it very closely resembled the C# 1 feature the <code class="language-plaintext highlighter-rouge">using</code>-statement. It is accompanied by an <code class="language-plaintext highlighter-rouge">AutoCloseable</code> interface, which corresponds to the <code class="language-plaintext highlighter-rouge">IDisposable</code> interface of .NET.</p> <p>This might have been an ideal time to make collection <code class="language-plaintext highlighter-rouge">for</code>-statement sniff the iterator for <code class="language-plaintext highlighter-rouge">AutoCloseable</code> so that iterators had a way to be notified in the event of an early <code class="language-plaintext highlighter-rouge">break</code> from the loop, but this didn’t happen.</p> <p>And after years of debate and delay, and against the wishes of a vocal subset of the user community, Java finally got lambdas, in its own way; again, the comparison is interesting. In C# there is the notion of a delegate, the type of something that can be called with <code class="language-plaintext highlighter-rouge">()</code>. Java resisted adding this, and instead noted that it is equivalent to an interface with one method (as long as it isn’t a generic method). Consequently a lambda could be used to implement any such interface.</p> <p>I really like this approach. Also it gives me a chance to complain about something that I think C# took in the wrong direction from the very start, and it’s too late to fix now, which will help to even up the bias in this post so far.</p> <p>I think using the function call <code class="language-plaintext highlighter-rouge">()</code> syntax directly on a type with one method should be equivalent to calling a method called <code class="language-plaintext highlighter-rouge">Invoke</code>, and that would remove the need for delegates to exist as a separate concept (this is so nearly the case; delegates do have methods, and one of them is <code class="language-plaintext highlighter-rouge">Invoke</code>.) This is an example of syntactic sugar compiling down to method calls, which is how many more recent features of C# work, but was underappreciated in the early days.</p> <p>A major difference between C# and Java lambdas is their ability to close over local variables. Both can do this, but only C# lambdas can capture a local (or parameter) that is mutable. The compiler has to do something rather strange to achieve this, moving the local variables into fields of a hidden class so they can continue their independent existence after the stack frame has vanished. Java decided against this level of concealed complexity and instead just refuses to compile a lambda that refers to a local variable whose value ever changes. Note that this cannot be defended as an example of some principled refusal to deal with mutable data, because Java will quite happily allow a lambda to read mutable fields of a object stored in a captured local.</p> <p>But as part of the same enhancement as lambdas, the tables were truly turned. Java did something it hadn’t managed for about 20 years: it introduced a new feature that would later be copied by C#. This was default interface methods, originally known as <a href="http://wiki.jvmlangsummit.com/images/7/71/2011_Goetz_Extension.pdf">virtual extension methods</a>, which were needed for the same purpose as C# static extension methods, but they are objectively <em>better</em>.</p> <p>A familiar problem when unit testing code that uses a package that includes extension methods is wanting to mock an interface, only to find that the method you’re calling is actually an extension on the interface, so it can’t be directly mocked. If only they’d used default interface methods! The same magic ability to make a new method appear on all existing implementations of an interface, but dispatch is properly polymorphic. But this excellent alternative wouldn’t appear in C# until version 9.0, five years later.</p> <h1 id="linq-and-streams">LINQ and streams</h1> <p>Here I’m conflating C# with the CLR, which these days is a reasonably safe thing to do. If you want to iterate through a collection you need something to represent the state of the iteration, and this needs to be an object distinct from the collection itself. In Java it’s called an <code class="language-plaintext highlighter-rouge">Iterator&lt;T&gt;</code>, while in C# it’s an <code class="language-plaintext highlighter-rouge">IEnumerator&lt;T&gt;</code>.</p> <p>From this perspective a collection is a thing from which you can obtain one of these iteration states, presumably initialised to the start of the collection. And sure enough, in Java a collection implements <code class="language-plaintext highlighter-rouge">Iterable&lt;T&gt;</code> (with its <code class="language-plaintext highlighter-rouge">iterator</code> method that returns an <code class="language-plaintext highlighter-rouge">Iterator&lt;T&gt;</code> ) and in C# it implements <code class="language-plaintext highlighter-rouge">IEnumerable&lt;T&gt;</code> (with <code class="language-plaintext highlighter-rouge">GetEnumerator</code> that returns an <code class="language-plaintext highlighter-rouge">IEnumerator&lt;T&gt;</code>.) So far, so isomorphic.</p> <p>But then the issue arises of how to provide helpful, composable operations on collections. The classic examples are <code class="language-plaintext highlighter-rouge">map</code>, <code class="language-plaintext highlighter-rouge">flatMap</code>, <code class="language-plaintext highlighter-rouge">filter</code> and <code class="language-plaintext highlighter-rouge">reduce</code>. In Java they have these exact names, where as in C# they are renamed <code class="language-plaintext highlighter-rouge">Select</code>, <code class="language-plaintext highlighter-rouge">SelectMany</code>, <code class="language-plaintext highlighter-rouge">Where</code> and <code class="language-plaintext highlighter-rouge">Aggregate</code>, by analogy with SQL (as if to put people off).</p> <p>But the real distinction is in where these operations appear. In C#, which gained them in 2007, they appear as extensions on <code class="language-plaintext highlighter-rouge">IEnumerable&lt;T&gt;</code>, that is, on collections, whereas in Java, which waited until 2014, they appear on a new object called a <code class="language-plaintext highlighter-rouge">Stream&lt;T&gt;</code> which:</p> <ul> <li>is be obtained from the collection by calling <code class="language-plaintext highlighter-rouge">stream</code></li> <li>can only be used once</li> </ul> <p>It seems a lot like another flavour of <code class="language-plaintext highlighter-rouge">Iterable&lt;T&gt;</code>. And you might expect a corresponding <code class="language-plaintext highlighter-rouge">Streamable&lt;T&gt;</code> to be implemented by collections with a <code class="language-plaintext highlighter-rouge">stream</code> method, and <a href="https://mail.openjdk.org/pipermail/lambda-libs-spec-experts/2013-February/001287.html">early in the development of this feature that was the case, but it was removed</a>. Could the operations have just been provided as default interface methods on <code class="language-plaintext highlighter-rouge">Iterator&lt;T&gt;</code>? It seems so.</p> <p>In any case, this is a fundamental difference but I’m not sure what to make of it. A C# method can accept a collection in the form of an <code class="language-plaintext highlighter-rouge">IEnumerable&lt;T&gt;</code>, and can therefore make multiple passes through it. A Java method that is passed a <code class="language-plaintext highlighter-rouge">Stream</code> can only make one pass. But the multi-pass capability is a double-edged sword. In C# when reviewing code I have occasionally discovered some very easy performance wins by adding <code class="language-plaintext highlighter-rouge">ToList()</code> here and there, to materialize an <code class="language-plaintext highlighter-rouge">IEnumerable&lt;T&gt;</code> that was being expensively re-evaluated. And conversely, I’m not sure I’ve wanted that kind of behaviour deliberately. I guess low memory conditions and very large data sets could be a compelling use case for lazy re-processing of a collection that doesn’t memoize the results of the first pass, but it seems odd that it’s the default in C#.</p> <p>Verdict: hmmm.</p> <h1 id="a-pattern-emerges">A pattern emerges</h1> <p>Over the last few years the two languages seem to have been adding very similar features at quite a pace. With pattern matching, C# 7.0 (2017) led the way. In its simplest form, an <code class="language-plaintext highlighter-rouge">if</code> statement can declare a variable:</p> <div class="language-cs highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">if</span> <span class="p">(</span><span class="n">someObj</span> <span class="k">is</span> <span class="kt">string</span> <span class="n">someStr</span><span class="p">)</span>
<span class="p">{</span>
    <span class="c1">// in this scope we know someStr is a string</span>
<span class="p">}</span>
</code></pre></div></div> <p>Likewise in Java 14 (2020), as a preview feature, you could say:</p> <div class="language-java highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">if</span> <span class="o">(</span><span class="n">someObj</span> <span class="k">instanceof</span> <span class="nc">String</span> <span class="n">someStr</span><span class="o">)</span> <span class="o">{</span>
    <span class="c1">// in this scope we know someStr is a string</span>
<span class="o">}</span>
</code></pre></div></div> <p>But C# went much further, sooner. Similar enhancements appeared at the same time in the <code class="language-plaintext highlighter-rouge">switch</code> statement, and tuples, and a form of destructuring to go with them (positional only, therefore). As with so many of these feature additions, neither language is inventing them; there are many pre-existing languages, often with quite niche communities, that have pioneered these features for years (even several decades), and both C# and Java are cherry-picking them for integration into their own, more popular, platforms and so bringing them to a much wider audience. Sometimes they do so in a strikingly identical way, thanks to commonalities in their basic implementations, and sometimes they are forced to diverge.</p> <p>Records have appeared in both languages also. Simplistically these are class-like and lean towards succinct declaration for a “primary” constructor, and toward immutability, and the provision of automatic implementations of equality comparison and hashcodes, i.e. they are value-like (by default, anyway.) But as with lambdas and closing over mutable locals, C# goes the extra mile even though it increases the complexity of its implementation. Java records cannot inherit implementation (only interfaces). But in C#, a record can inherit another record, and this means the language has to be careful about what equality means. Also there’s the delightful <code class="language-plaintext highlighter-rouge">with</code> expression that can perform a non-mutating update, and which also strives to provide non-surprising behaviour under inheritance. The expression:</p> <div class="language-c# highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="k">with</span> <span class="p">{</span> <span class="n">FirstName</span> <span class="p">=</span> <span class="s">"Joe"</span> <span class="p">}</span>
</code></pre></div></div> <p>produces a modified clone of the runtime type of <code class="language-plaintext highlighter-rouge">x</code>, which may differ from the compile-time type, so polymorphism is properly honoured.</p> <h1 id="whats-next">What’s next?</h1> <p>Probably the most interesting recent C# feature is <a href="https://learn.microsoft.com/en-us/dotnet/csharp/whats-new/tutorials/static-virtual-interface-members">static virtual interface members</a>, which greatly increases the expressiveness of generic types. In mathematics when we introduce an operator closed over a set, we often introduce special elements of the set like a “zero”. Now we can write such concepts down in C#. Or thinking more practically, we can do much of what C++ traits have always been able to do.</p> <p>Your move, Java!</p>]]></content><author><name></name></author><category term="languages"/><category term="c#"/><category term="java"/><summary type="html"><![CDATA[A pithy quote:]]></summary></entry><entry><title type="html">Domesday ‘86 Reloaded (Reloaded)</title><link href="https://danielearwicker.github.io/blog/2021/Domesday/" rel="alternate" type="text/html" title="Domesday ‘86 Reloaded (Reloaded)"/><published>2021-02-07T00:00:00+00:00</published><updated>2021-02-07T00:00:00+00:00</updated><id>https://danielearwicker.github.io/blog/2021/Domesday</id><content type="html" xml:base="https://danielearwicker.github.io/blog/2021/Domesday/"><![CDATA[<p>TL;DR: <a href="https://earwicker.com/dansday86/">I built this yesterday</a>.</p> <p>Back in the mid-1980s my primary (same thing as elementary) school suddenly told us we had to come up with content for <a href="https://en.wikipedia.org/wiki/BBC_Domesday_Project">a digital encyclopedia of the UK</a>, conceptualised as a successor to William I’s audit of his freshly conquered territory carried out exactly 900 years earlier. Yes, like the crowd-sourced Internet utopia envisaged in the mid-2000s, “Web 2.0”, in which ordinary people are both the content providers and consumers, and will become empowered, and definitely won’t turn into Nazis and storm the citadels of democracy. All the good parts of that somehow travelled back in time to 1985 (Marty). Except it wasn’t really grass-roots, it was astro-turf: the BBC conspired with the schools to <em>make</em> it happen, in a top-down patrician Lord Reith type of way.</p> <p>Even so, to my childhood mind, it’s like I’m Ford Prefect doing field research for The Guide and unfortunately I’ve been stranded on this pathetic little planet for slightly longer than expected and I’m not really from Guildford after all (or indeed ever).</p> <p>Then a year or so later the project was finished, and it was published on a couple of <em>laser discs</em> (nothing will ever sound more futuristic and cheesy) that could only be interactively explored on some expensive modified hardware. But it was in a way a groundbreaking exercise that seems now like it was made of uncanny magical reverse echoes of trends that would not truly emerge for decades.</p> <h2 id="digital-obsolescence">Digital Obsolescence</h2> <p>Domesday ‘86 has since become the widely referenced example of digital obsolescence, mainly because to anyone looking to make a Luddite point, the obvious comparison is just so irresistible: the original Domesday book was still readable because paper is forever. This is of course nonsense: paper rots, is scribbled on, gets soggy, burns (okay maybe not both of those at once). Maintaining something in a readable form requires effort and attention, even if that means keeping it in a special room, away from children, floods or flames, touched only by people wearing those fancy white science gloves. Nothing lasts forever unless we lavish care on it. If a thing disappears, it’s because no one cared.</p> <p>And as it turns out, people did care about Domesday ‘86. <a href="http://www.ariadne.ac.uk/issue/36/tna/">Adrian Pearce’s heroic efforts</a> produced a recovered, fully digitised dataset (originally the images on the discs were just analogue video frames), which he made available online, but it disappeared on his death in 2008. In any case, the BBC held the copyright and 2011 they spun up a project to give the material a modern (well, 2011) web-based UI. But then they pulled the plug on that site, entrusting the material to the National Archive. Meanwhile a <a href="https://www.domesday86.com/">really hardcore authentic preservation/revival Domesday ‘86 project</a> continues, aiming to emulate the original software and user experience as closely as possible.</p> <p>So a happy ending? Well, not quite. <a href="https://twitter.com/mumoss/status/1011904564910608384">As many have discovered</a>, when the National Archive made a copy of the BBC’s site, they just webcrawled it to make <a href="https://webarchive.nationalarchives.gov.uk/20120116144044/http://www.bbc.co.uk/history/domesday">a static mirror, and stuck it online</a>. None of the interactive elements work, except that if you start from a content page for an actual place, you can click links to go North, South, East or West, so it’s like a classic text adventure game: tedious and irritating.</p> <p>It’s particularly galling that they also put a running clock’s timestamp in the URL of each page as they copied it, e.g.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>https://webarchive.nationalarchives.gov.uk/20120316094313/http://www.bbc.co.uk/history/domesday/dblock/GB-348000-534000
</code></pre></div></div> <p>Right before the original BBC URL there’s <code class="language-plaintext highlighter-rouge">20120316094313</code>, and as if to maximise the frustration, this is different across the pages. By the look of it, they backed up that page On March 16th, 2012 at 09:43 and 13 seconds. Why is this bad? Because apart from that stupid timestamp, this is very nearly a deterministic URL. The <code class="language-plaintext highlighter-rouge">GB-348000-534000</code> tells us the Ordnance Survey map coordinates for the south-west corner of the region covered by that page. But we can’t make use of that fact unless we also know the exact time that they backed up that specific page. Ridiculous.</p> <h2 id="the-idea">The Idea</h2> <p>It would be really cool if we could browser a modern map UX (i.e. Google Maps) on which there was layer of rectangles showing where the Domesday ‘86 project had data, and we could click on them to go to the official archive mirror page for that area. This would combine several advantages over conceivable alternatives:</p> <ul> <li>Inherent ability to compare today’s landscape with The Past.</li> <li>No copyright issues: the map UX would only need the URLs of the Domesday archive pages, not any of the actual content.</li> <li>No need for any server backend!</li> </ul> <p>That last point needs finessing a bit. With “modern” web technology it can just be a static site that can be deployed onto a webserver, downloaded into browsers, which can then do all the magic. Okay, yes, it depends on Google maps continuing to exist, and also browsers. But so does a lot of the economy, which means people care about those things, which means they’ll continue to exist. Just gotta piggyback on things that already have that quality of being-cared-about for the best chance of survival.</p> <p>So, to business.</p> <h2 id="crawling-the-mirror">Crawling the Mirror</h2> <p>We need to get around that timestamp problem. The only way to do this is to write a new web-crawler. We can start on any page, download the HTML, and look for the links:</p> <p><img src="/resources/dansday86/links.png" alt="Links to North, South, East, West"/></p> <p>That gives us up to four more pages to download, and we can discard any we already know about. Proceeding like this, we will effectively crawl outwards from the starting point until we reach the edge of the map. As the program crawls, it can just output URLs to a log file. I write each URL twice: once when I discover its existence from a link, and then again when I’ve actually downloaded its content.</p> <blockquote> <p>Pompous Aside 1: This is a really good engineering concept: the append-only file format. It’s incredibly robust. If the crawler quits unexpectedly then the log file tells us exactly how far it got and it can pick up where it left off.</p> </blockquote> <p>In the log file, I prefix each URL with a letter:</p> <ul> <li><code class="language-plaintext highlighter-rouge">Q</code> - it’s been “queued” for downloading after I found it in a link</li> <li><code class="language-plaintext highlighter-rouge">A</code> - “absent”: the page had a heading telling us that there’s no content</li> <li><code class="language-plaintext highlighter-rouge">P</code> - “present”: it’s got great 1986 content we all want to read</li> <li><code class="language-plaintext highlighter-rouge">E</code> - “edge”: got a 404 status from the download attempt</li> </ul> <p>So each line is just a capital letter, a space, and a URL.</p> <blockquote> <p>Pompous Aside 2: I could have parsed the URL into the bare numbers (the coordinates and the timestamp). But then I’d need to invent a format to write to the file. The URL is already such a format; yes, it’s got tons of repeated baggage in it, but it’s perfectly adequate. Don’t rush to do something fancy and “proper” in the early stages of a project, because you’ll probably just make it more complicated than necessary, to little advantage.</p> </blockquote> <p>When I come to read the log, I can just take the <code class="language-plaintext highlighter-rouge">P</code> lines and ignore the rest. So I started the crawler running, with no throttling to avoid overloading the server. If you’re from the National Archive and you’re wondering why you had a weird traffic spike last week, now you know. Sorry.</p> <p>I believe I started at the URL for Barnard Castle, a location held in special affection by the British. But I had to do the whole thing twice, because as well as URLs with <code class="language-plaintext highlighter-rouge">GB</code> coordinates, there are others with <code class="language-plaintext highlighter-rouge">NI</code> (Northern Ireland), and they don’t join up, but in an interesting way that I promise has nothing to do with any trade border down the Irish Sea.</p> <h2 id="rendering-a-map-because-why-not">Rendering a Map, Because Why Not?</h2> <p>To pass the time as I impatiently waited for results, and to convince myself it was working, I wrote another script that would scan the logfile so far and render an image of the map.</p> <blockquote> <p>Pompous Aside 3: a really good way to keep up your enthusiasm for a project in the early stages is to make it visual. Give yourself a way to see your machine working. A dry text log file is not an exciting or rewarding form of feedback. Turn numbers into graphics, animations, cool stuff. Visualise the machine.</p> </blockquote> <p>How did I do that? Easy - the <code class="language-plaintext highlighter-rouge">GB</code> coordinates in the URL are really simple. Each page relates to an area 4km (East-West) by 3km (North-South). Why that aspect ratio? So it would fit on an old computer screen. These days it would be 8km by 4.5km to get a 16:9 ratio. If you divide the URL’s coordinates by 4000 and 3000, you get <code class="language-plaintext highlighter-rouge">(x, y)</code> integer coordinates of a grid cell. So my render script would read the log and colour in pixels according to the current state letter. Here’s an early run of it:</p> <p><img src="/resources/dansday86/map2.png" alt="Weird diamond shape bitmap"/></p> <p>Note the diamond pattern (well, square rotated 45 degrees) - this is exactly what you’d expect from an outward crawl by the method I describe above. Also there’s a lovely crinkly edge to the West, because that’s the actual edge of the map (red pixels indicate <code class="language-plaintext highlighter-rouge">E</code> results from the log file). Here’s one from a while later:</p> <p><img src="/resources/dansday86/map5.png" alt="Clearly part of Great Britain"/></p> <p>That’s clearly dear old Blighty! And finally:</p> <p><img src="/resources/dansday86/map-gb.png" alt="All of Great Britain"/></p> <p>It’s stretched vertically because my pixels are square, which is too tall because the area each pixel represents is a 4:3 oblong. Also there is a suspicious preponderance of green <code class="language-plaintext highlighter-rouge">P</code> pixels on the border where there aren’t red ones. I suspected (correctly as it turns out) that this was a lie caused by a bug in my crawler: as well as the <code class="language-plaintext highlighter-rouge">A</code> pages that say they have no content (yellow), there are ones that say they’re off the edge of the map. I should have classified those as <code class="language-plaintext highlighter-rouge">A</code> also. Darn it.</p> <p>Northern Ireland appeared relatively quickly:</p> <p><img src="/resources/dansday86/map-ni.png" alt="All of Northern Ireland"/></p> <p>Not gonna lie, it was mildly thrilling to see those shapes appearing from the raw data in real time. But what to do with them next?</p> <h2 id="maps-are-complicated">Maps Are Complicated</h2> <p>Drawing flat maps of a curved surface is really complicated. By the way if you subscribe to alternative theories of the shape of the Earth, stop reading now, because this will get upsettingly reality-based. Nowadays a global standard coordinate system has taken over: <a href="https://en.wikipedia.org/wiki/World_Geodetic_System">WSG 84</a>. Every point on the earth is identified by two numbers:</p> <ul> <li><em>longitude</em> is measured in degrees with the zero at the Greenwich Meridian, because… colonialism.</li> <li><em>latitude</em> has the zero at the equator and +90 degrees at the North Pole, -90 degrees at the South Pole.</li> </ul> <p>In a smart phone’s browser there’s an API you can call to get those two numbers. And in Google maps, you use those two numbers to specify a location. It all joins up nicely, and is simple to understand if you don’t look too closely at the details (very much the UTC of coordinate systems, including the Greenwich origin.) And as the tectonic plates drift around the globe (seriously, Hawaii actually moves a metre every decade) there is a worldwide effort to keep maps up-to-date with this system. The land moves, but the coordinates stay still, in some weird negotiated sense.</p> <p>It works great for most places where people gather in large numbers, because near the equator North-South lines are <em>almost</em> parallel. They start to converge as you get closer to the poles, so travelling “1 degree West” is a significantly longer journey in London than it is in Edinburgh. And at the poles the distortion becomes infinite and the longitude becomes meaningless. But who hangs out there? No one important.</p> <p>Even so, on a local scale, historically, other coordinate systems were used. For the UK, <a href="https://www.ordnancesurvey.co.uk/business-government/tools-support/os-net/coordinates">Ordnance Survey has a fantastic document</a> that is well worth at least skim reading. There’s also <a href="https://epsg.io/">the EPSG</a> repository of worldwide coordinate systems, which gives each one an ID number, in accordance with bureaucratic tradition. The ones we care about here are:</p> <ul> <li><a href="https://epsg.io/4326">EPSG:4326</a> - the WSG 84 global standard.</li> <li><a href="https://epsg.io/27700">EPSG:27700</a> - the British National Grid, defined in 1936.</li> <li><a href="https://epsg.io/29901">EPSG:29901</a> - the Irish National Grid (1952).</li> </ul> <p>The last two appear in the URLs with the GB and NI prefixes. We need to convert them to WSG 84, and that’s a fiddly mathematics problem. The excellent OS document shows what we’re dealing with:</p> <p><img src="/resources/dansday86/coord-mapping.png" alt="WSG 84 and BNG overlaid"/></p> <p>Fortunately <a href="https://www.npmjs.com/package/transform-coordinates">smart people already figured all this out</a> and I can just pass the GB/NI coordinates and get back global long/lat coordinates. Well, almost. Any 40km-by-30km cell in the National Grid can be defined by two numbers, giving the location of the South-West corner. But in WSG it will be deformed by the transformation. It won’t be a rectangle. Strictly speaking it won’t even be a regular polygon because the edges will be curves. It will be the opposite of the above composite map: the WSG lines will be straight, so therefore National Grid lines will have to curve.</p> <p>But our cells will still have four corners, and for simplicity I’m going to model them as regular polygons, so the grid will become a mesh of four-side polygons. To define each cell, I need four sets of coordinate pairs, giving the positions of the corners, which won’t line up along lines of latitude/longitude.</p> <h2 id="google-maps-are-easy">Google Maps are Easy</h2> <p>Eager to see if my coordinate conversions were working, I spun up a simple Google Maps project, basing it on the fantastic <a href="https://parceljs.org/">Parcel</a> bundling engine. Honestly, give it a try. It’s like a dream version of Webpack.</p> <p>Google Maps makes it very easy to draw polygons on the map: they have a specific API for that, and you just give it a list of longitude/latitude pairs. I initially did this with the individual cells, of which I think there are about 30,000. It took a while to open, and the zooming was painful. Clearly I needed to be a bit smarter about it than that. But it was an important exercise because it gave me confidence that I was heading in the right direction, and that all-important visual feedback.</p> <blockquote> <p>Pompous Aside 4: Try the simple, dumb approach. You never know, it might work. If it doesn’t, you’ll find out what you need to know to fix it.</p> </blockquote> <p>What we have here is a scaling problem. 30,000 polygons aren’t individually meaningful, they’re too fine grained. We need bigger grains. Let’s divide the original grids up into 400km-by-300km “zones”, each containing 100 of the original cells. We should then end up with ~100 zones, and we can render those as polygons, and paint them with an opacity that is just the number of cells in the zone divided by 100 to get a value between 0 and 1. That gives us a heat map of the density of cells in each region, so at the national zoom level we can immediately see the areas with the most information.</p> <p>Then when the user zooms in closer, we can switch over to rendering the individual cells’ polygons, but only for the zones that overlap with the currently visible portion of the map, so we don’t overwhelm the platform. If you zoom in to that level and drag the map around, the code continuously “hit tests” which zones are now visible and creates or destroys their polygons accordingly.</p> <blockquote> <p>Pompous Aside 5: Very often, the solution to a scaling problem consists of using a different design at different scales. Don’t expect one general approach to scale smoothly. Systems that scale impressively often achieve this by having discontinuities, points where they switch between different modes as the scale increases. Maps themselves are a good analogy: national grids are more convenient for giving coordinates locally, and are still used for that reason, but they stop working at the global scale.</p> </blockquote> <p>The data for all the zones and cells is actually linked into the source as a JSON file, meaning that the site doesn’t need to call a custom server to download extra data. I was able to cut the size of the JSON file by over 80% by compressing each zone’s list of cells, using the <a href="https://www.npmjs.com/package/lz-string">lz-string</a> package’s handy base64 format. Every time a zone scrolls into view, I decompress that zone’s cell list on the fly.</p> <p>Finally, when the user clicks a cell, we can navigate to the original archived page for that cell. One last cool thing I added this morning was browser history support, something that is very important for any web app. When the user clicks the Back button, we need to go back to where they were. Sometimes the meaning of “where we are” (in an app’s UX) is ambiguous and open to debate, but when it comes to showing places on a map, clearly we are dealing with the archetypal concept of “where”. So right before navigating to the archive site, I use <code class="language-plaintext highlighter-rouge">history.pushState</code> to capture the following in the address bar:</p> <ul> <li>the map’s zoom level</li> <li>the longitude/latitude centre point of the map</li> <li>the GB or NI coordinates from the URL we’re about to navigate to.</li> </ul> <p>Then when the site loads up, I grab that info and obey it (I use the coordinates to show the last-visited cell in a different colour).</p> <p><a href="https://earwicker.com/dansday86/">And here it is!</a></p> <p>I decided to call it Dansday ‘86, in honour of Dean Pelton from Community (except I <em>Chang</em>-ed it to my name instead of his.)</p> <p>The End.</p>]]></content><author><name></name></author><category term="maps"/><category term="archival"/><category term="bbc"/><summary type="html"><![CDATA[TL;DR: I built this yesterday.]]></summary></entry><entry><title type="html">The Blob Lottery</title><link href="https://danielearwicker.github.io/blog/2020/The-Blob-Lottery/" rel="alternate" type="text/html" title="The Blob Lottery"/><published>2020-09-27T00:00:00+00:00</published><updated>2020-09-27T00:00:00+00:00</updated><id>https://danielearwicker.github.io/blog/2020/The-Blob-Lottery</id><content type="html" xml:base="https://danielearwicker.github.io/blog/2020/The-Blob-Lottery/"><![CDATA[<p>The simplest, cheapest and fastest form of storage in the cloud is the blob. It’s very bare-bones, making no attempt to compete with more high-level searchable storage offerings that help you by making your data searchable every which way. It’s little more than a remote file system. But if you can put up with those limitations, you can save $$$.</p> <p>Today I’m going to consider the question: if we have a dataset that we want to store in the cloud, how far should we go in breaking it down into pieces? However we decide to organise the data (indexed, sorted or just however-it-comes), there are good reasons to want to break it into pieces. Regardless of any other choices we might make, I want to see what impact this “granularity” decision will have.</p> <p>My particular use case involves a dataset of many millions of items, of which thousands are updated during a nightly “processing run”. A naive first guess is that I should arrange cut the data into small enough pieces so that each of these nightly batch updates is required to read and write a minimal subset. The fewer raw bytes I have to transfer over the network, the faster my process should go, right?</p> <h2 id="worst-case-scenario">Worst case scenario</h2> <p>In many lucky scenarios you can take advantage of patterns in how data is accessed, known as <a href="https://en.wikipedia.org/wiki/Locality_of_reference">locality of reference</a>. Building your design around such assumptions can give you a huge advantage, e.g. if a blog post is requested, we can assume the comments for that blog post are about to be requested very soon, so we should store them near to each other. Sometimes the past <em>is</em> a guide to the future!</p> <p>But here I’m going to be pessimistic and assume no such pattern can be found: the daily update selects around one in a thousand items entirely at random out of the full set. Where there’s randomness, there’s probability, which means our intuitions tend to skip important details and get wrong answers.</p> <h2 id="randomness">Randomness</h2> <p>In the tradition of rolling a die, suppose the dataset is divided into 6 equally sized pieces which we’ll call pages. If you pick a random spot to update, that spot is equally likely to appear in any page, so the probability of a page being hit is 1/6. What about the probability of a page being hit at least once after two updates?</p> <p>Here there’s a temptation to say 2/6, like the number on the top of the fraction is the number of updates. This is our intuition, and maybe we tend toward that guess because it happens to be a good approximation if the number on the bottom of the fraction is much larger. But it isn’t right, because there’s a 1/6 chance that the second update will land on the same page as the first. The more pages are hit, the fewer un-hit pages remain, so the likelihood of a collision increases. The clue is in the pesky <em>“at least once”</em>. To start with, all pages have equal probability of being hit, but once a page is hit, it stays hit, so the probability of it transitioning from un-hit to hit again on subsequent updates collapses to zero.</p> <p>This kind of problem is easier to solve if you flip it around. If a page avoids being hit several times in a row, those are truly independent events. A page that remains un-hit is still in the game. The probability of a page <em>not</em> being hit by one update is 5/6. If it survives two unscathed, that’s two independent events and we can multiply the probabilities, 5/6 * 5/6, or 25/36. We can then subtract that from 1 to get the probability of the page being hit at least once. More generally the probability of each of <code class="language-plaintext highlighter-rouge">N</code> pages being hit after <code class="language-plaintext highlighter-rouge">U</code> updates is:</p> \[P = 1 - \left( \frac{N-1}{N} \right)^U\] <p>This is equivalent to the fraction of <code class="language-plaintext highlighter-rouge">N</code> pages we could typically expect to be hit after <code class="language-plaintext highlighter-rouge">U</code> updates.</p> <p>The inputs to the formula are integers, so it’s a bit clunky for small values, but once you get into thousands of updates it smooths out into the same consistent curve at all scales:</p> <p><img src="/resources/pages-hit-probability.png" alt="graph of probability curve"/></p> <p>The x-axis is <code class="language-plaintext highlighter-rouge">N/U</code>, so 10 means the number of pages is 10 times the number of updates. As you can see, if the number of pages equals the number of updates, most of the pages will be hit, and you need around 10 times as many pages to keep it down to 10% of pages being hit.</p> <h2 id="what-this-tells-us">What this tells us</h2> <p>Why is this important? There are two potential performance costs involved here. If <code class="language-plaintext highlighter-rouge">N</code> is small, the nightly operation will read/write large numbers of irrelevant items because they happen to live in the same pages as relevant items. Our intuition says moving large amounts of data unnecessarily is wasteful so we feel an urge to avoid that. On the other hand, if <code class="language-plaintext highlighter-rouge">N</code> is large, we perform more separate physical read/write operations, because our data has been diced into many tiny blobs. The question is, which of these is more costly?</p> <p>Back in olden times, when our data was stored in hard drives, there was the time taken to read data, mentioned in the ancient chronicles as 20ms/MB, but legend also told of a so-called seek time of 10ms. This meant that if you broke a megabyte of data up into 10 pieces that were scattered around the drive, it would take 120ms to read it all - the seek time overhead was huge. Maybe the overhead of dealing with many individual blobs will work like the seek time overhead. But this will obviously depend on how the blob storage service has been implemented. And there are lots of details about the internals of that service that we don’t know.</p> <p>So to answer that question, we may as well do an experiment.</p> <h2 id="experiment-time">Experiment time!</h2> <p>I’m going to use Azure Blob Storage because I have plenty of allowance to play with. The test code will run from a VM deployed in the same Azure region, to give me the fastest possible access to the storage service. This is really important - running a test like this from your laptop at home gives a very misleading impression. Inside the data centre the networking is state-of-the-art.</p> <p>Suppose each item/record/whatever is 500 bytes, and there are 10 million items, so 5GB of data. Every so often I have to apply a wave of updates to a random 10,000 items. First, following my intuition, I’ll try to keep the number of accessed pages down to 10%. So I know from the above formula I need to divide my items into 100,000 pages of 100 items. As we’ll only be touching 10% of the pages, that will be 10,000 physical read/writes. To test both directions I will literally save a chunk of data equivalent to a page size, and then read it back again (this is the opposite order to a real update, but hopefully represents about the same amount of total work).</p> <p>But what if I didn’t care how much data is read? So that we don’t have to hold all the data in memory, we still partition it, but this time into a mere 100 pages. This is only 1% of the number of updates, and so this pretty much guarantees all the pages will be hit, but there will only be 100 physical read/writes.</p> <p>Also, the formula tells us it’s not going to help us if we go for 1000 pages - it may sound like a lot, but this is only a tenth of the number of updates, and all the pages will still be hit. So if there’s a seek time-like overhead, this is going to be a terrible choice for the number of pages.</p> <table> <thead> <tr> <th style="text-align: right">Pages</th> <th style="text-align: right">Page size</th> <th style="text-align: right">Pages hit</th> <th style="text-align: right">Read/written</th> <th style="text-align: right">Seconds</th> </tr> </thead> <tbody> <tr> <td style="text-align: right">1</td> <td style="text-align: right">5 GB</td> <td style="text-align: right">1</td> <td style="text-align: right">5 GB</td> <td style="text-align: right">66</td> </tr> <tr> <td style="text-align: right">10</td> <td style="text-align: right">500 MB</td> <td style="text-align: right">10</td> <td style="text-align: right">5 GB</td> <td style="text-align: right">65</td> </tr> <tr> <td style="text-align: right">100</td> <td style="text-align: right">50 MB</td> <td style="text-align: right">100</td> <td style="text-align: right">5 GB</td> <td style="text-align: right">68</td> </tr> <tr> <td style="text-align: right">1,000</td> <td style="text-align: right">5 MB</td> <td style="text-align: right">1,000</td> <td style="text-align: right">5 GB</td> <td style="text-align: right">107</td> </tr> <tr> <td style="text-align: right">10,000</td> <td style="text-align: right">500 KB</td> <td style="text-align: right">6,321</td> <td style="text-align: right">3.16 GB</td> <td style="text-align: right">171</td> </tr> <tr> <td style="text-align: right">100,000</td> <td style="text-align: right">50 KB</td> <td style="text-align: right">9,516</td> <td style="text-align: right">476 MB</td> <td style="text-align: right">137</td> </tr> <tr> <td style="text-align: right">1,000,000</td> <td style="text-align: right">5 KB</td> <td style="text-align: right">9,950</td> <td style="text-align: right">49.8 MB</td> <td style="text-align: right">137</td> </tr> <tr> <td style="text-align: right">10,000,000</td> <td style="text-align: right">500 B</td> <td style="text-align: right">9,995</td> <td style="text-align: right">5 MB</td> <td style="text-align: right">134</td> </tr> </tbody> </table> <p>Well. As the number of pages increases, it becomes more and more likely there will be no collisions, until the number hit is roughly equal to the number of updates made (which, remember, is always 10,000). But the only thing we really care about is the total time taken. Basically we can shift 5 GB in both directions in just over a minute, as long as we don’t break it up into more than a 100 or so pieces. That’s only 1.33 Gbps, so slower than the raw read speed of an SSD, but we are writing too, so pretty impressive.</p> <p>But as we try to break it into smaller pieces in an attempt to cut the amount of data we have to transfer, the running time goes up, and while it comes down a little, it’s at least twice as slow as reading all the data. This is true even when we’re only reading 1/1000th of the dataset.</p> <h2 id="why-even-have-lots-of-pages">Why even have lots of pages?</h2> <p>Of course, this does not mean that all systems that break data into smaller pieces are stupid. And this, by the way, includes popular databases that typically have quite small page sizes. In Microsoft SQL Server it’s a measly 8 KB, which implies 625,000 pages, and would certainly be a poor size given the assumptions in my example scenario (even if not using cloud storage). But my example has been chosen specifically to illustrate a situation where such granular pages are not helpful, simply because my regular bulk processing runs will access 10,000 records at random from across the whole dataset, as quickly as possible. This is different from how a traditional RDBMS is typically used, where 1 or 2 records from the entire database are requested at a time by individual users, and also their access patterns are distinctly non-random. The majority of requests are aimed at recently stored records, and so the requests of current users will fall heavily on a few hundred pages that can be cached cheaply. And caching works wonders when writes are rarer than reads, which they very often are. But in my scenario, there are equal numbers of writes and reads, and the cache would constantly be defeated by randomness.</p> <h2 id="why-even-have-more-than-one-page">Why even have more than one page?</h2> <p>Supposing my scenario is applicable, and so lots of small pages would be counter-productive: why even bother to split the dataset at all? The obvious reason is that it may not fit conveniently into memory on a single server. 5 GB is not a huge amount of memory these days, but a cloud node that supports 50 GB is still pretty expensive. If you can segment the data, you have way more flexibility.</p> <p>The other important reason is to support parallel processing. Break the data up into 10 to 100 partitions and you can (temporarily) spin up that many nodes to separately perform the updating. Each partition is like a little separate world. The good news about blob storage services is that they elastically scale, meaning that as you throw more parallel requests at them, they just keep serving at the same speed per request. Want your processing to go 100 times faster? Just do 100 pieces of it at the same time.</p> <p>Of course it’s not entirely that simple: what if the information that arrives to tell us what updates to make is also in random order? We’ll need to route each piece of information the relevant partition. And we don’t want to be doing that in tiny pieces, given what we know about the performance implications. If we receive 100,000 new versions of records to store, we need to split them into 100 parcels of 1000 records, one for each of our 100 partitions. The solution is to run a non-parallel first pass that constructs the input parcels as blobs, before we launch the all the parallel processing.</p> <p>To get the most out of parallel processing, we need to ensure that records are evenly divided between the partitions. The records will be accessed by some key, but we don’t know in advance how that key’s values will be distributed (or whether any pattern will persist over time). There are a number of strategies, but if the access pattern is truly random, we will do no real harm by shuffling the records any way we like. And so we can use the simplest approach of all, which is to simply hash the key to an integer and then scale it down to the number of partitions. The advantage of this is simplicity. We don’t want to write our own database, we just want to store stuff in blobs with minimal logic required to locate the data we require. The logic described here is a few lines of code.</p> <p>By the way, for the kind of approach I’m using here, Azure’s “hot” tier is appropriate, because you are not charged for the amount of data you transfer, only the total amount stored, and also a very small charge based on the number of individual requests you make. So there is even a financial incentive to make fewer requests for larger pieces of data, rather than more requests for smaller pieces.</p> <p>Even so, we <em>could</em> get more sophisticated. A single index blob could be used to describe ranges of key values belonging to partitions, mapping them to integer IDs of blobs containing their records. When partitions get too large it will be necessary to split them. Supposing they shrink, we’ll have to merge them. The advantage of this is that where access to keys is not random, but instead turns out to exhibit locality of reference, we’ll find that a given update wave will hit far fewer pages than predicted by the random model, as the updated records will often be clustered together in the same few pages.</p> <p>But has to be set against the downside: more complex code. We are drifting towards implementing a custom B+tree. And it probably isn’t worth it, given the elastic scaling power of blob storage. We can churn through our 5 GB of data in just over a minute, or 150,000 records per second, even if we do it serially. If we do 10 in parallel, it goes up to 1.5 million records per second. If necessary, we can scale out to 100 partitions before any significant overhead is encountered, which seems to imply we can cover the whole dataset in under a second! This of course depends on whether the storage service is really <em>that</em> elastic, and how network traffic is routed inside the data centre. I haven’t tested that yet…</p> <p>Probably worth seeing how you get on with that kind of raw speed before committing a more complex design.</p>]]></content><author><name></name></author><category term="cloud"/><category term="probability"/><summary type="html"><![CDATA[The simplest, cheapest and fastest form of storage in the cloud is the blob. It’s very bare-bones, making no attempt to compete with more high-level searchable storage offerings that help you by making your data searchable every which way. It’s little more than a remote file system. But if you can put up with those limitations, you can save $$$.]]></summary></entry><entry><title type="html">Abstraction is a Thing</title><link href="https://danielearwicker.github.io/blog/2020/Abstraction-is-a-Thing/" rel="alternate" type="text/html" title="Abstraction is a Thing"/><published>2020-03-07T00:00:00+00:00</published><updated>2020-03-07T00:00:00+00:00</updated><id>https://danielearwicker.github.io/blog/2020/Abstraction-is-a-Thing</id><content type="html" xml:base="https://danielearwicker.github.io/blog/2020/Abstraction-is-a-Thing/"><![CDATA[<p>When aliens finally pay us a visit and they start floating around our cities, struggling to pronounce greetings from a phrasebook, we will no doubt say to one another, “Apparently aliens are a thing now.” When we recognise something has started happening all the time, we call it “a thing”. Or we might remind our friend in a tone of heavy irony, after they accidentally walk into a lamppost, “Yup, lampposts are still a thing.”</p> <p>Of course, deep down every “thing” is just subatomic particles and forces. There is nothing else. Except of course there is! It’s a frustrating thing about casual pop science explanations that they stray into that kind of obsessive reductionism. Things don’t stop existing just because you found out what they’re made of. No one seriously stops referring to chairs and tables when they learn about atoms. I can’t put it better than Stephen Pinker:</p> <blockquote> <p>Good reductionism (also called hierarchical reductionism) consists not of <em>replacing</em> one field of knowledge with another but of <em>connecting</em> or <em>unifying</em> them.The building blocks used by one field are put under the microscope of another.</p> </blockquote> <p>The things we identify, and speak in terms of, may not be of fundamental importance to the universe, but they are important to us. We explain what’s going on in the world in terms of helpful concepts, of appropriate scale. We’re not just going to <em>exist</em> in the world; we’re trying to <em>understand</em> the world. That’s what all these things are for.</p> <p>Constructing software is very like this. The “atoms” of software might be operations on values in memory or registers. Explaining how a large program works by throwing the machine code instructions up on a projector, and scrolling through them while saying “See?! I told you it was simple!” is not usually that helpful. We rarely even see these atoms; they are the output of a compiler. Instead we deal with goodly-sized things.</p> <h2 id="reality-and-existence">Reality and Existence</h2> <p>You may counter this by pointing out that the things inside software aren’t real, and they don’t actually exist. And in a quite boring sense, you are correct: once the software is running, it boils down to billions of those little individual operations on values in memory, nothing else.</p> <p>But you’re making the same mistake as the obsessive reductionist, throwing around words like “real” and “exist” like you own them. We’re not talking about what the software needs in order to merely exist; we’re talking about what we need in order to understand it. That’s a real need: understanding, intentions, purpose, especially shared across multiple authors, are all vitally important here. This is how we arrange for the right individual operations to happen in the right order at runtime. There are patterns in, and constraints on, what those operations will do. That’s what we all need to have shared knowledge of, if we’re going to collaborate on improving and extending a software product.</p> <p>This is why we construct software in terms of abstractions (the fancy name for “things”) and we want to invent new ones. We’re trying to make the software’s internal structure easy to understand, so it is easily picked up and used as a starting point for the next person who has to deal with our… stuff.</p> <p>The least controversial abstractions are the ones that seem to leap out at us from the world of ideas, and dance back and forth shouting “Here I am! I exist!” They have an obvious concreteness. If I wanted to sound artsy-fartsy I’d called them platonic ideals, but there’s nothing clever about them. Well, sometimes they can have subtleties (very different from subtitles). But even so, it’s incredibly easy to spot them in the wild. <a href="https://earwicker.com/carota/">In my text editor, Carota</a>, there’s a thing called a <a href="https://github.com/danielearwicker/carota/blob/master/src/word.js">word</a>. We all know text (in most languages) is made of words, so it’s not at all surprising to find them in the code having a kind of independent existence.</p> <p>On the other hand, Carota’s words have an interesting kind of inner life, a structure of their own. I decided that a word would have two <em>sections</em>, first a run of text (non-space) characters, and second a run of space characters, such that the space between two words “belongs” to the first. Even if you type some spaces at the start of a document, that’s treated as a word whose text section is of length zero. Why did I do it that way? It has to do with the way words wrap onto new lines. And this is of fundamental importance: words exist for the purpose of wrapping the text onto lines - they act as units of wrapping. In a text editor you can type characters however you like, without thinking of organising them as words. You can select a range of characters that spans multiple words, and cuts words in half, and then say “Make that part bold”. So at this level of explanation, <em>words don’t exist</em>. We summon them into existence when we need them.</p> <p>There is more than one correct way of looking at a text document. After all, it’s “just” a stream of characters. Or maybe it’s a stream of <a href="https://github.com/danielearwicker/carota/blob/master/src/runs.js">runs</a>, each run being a group of characters having the same formatting. Or maybe it’s a hierarchy of <a href="https://github.com/danielearwicker/carota/blob/master/src/line.js">lines</a> containing <a href="https://github.com/danielearwicker/carota/blob/master/src/positionedword.js">positioned words</a> that in turn contain positioned characters, supporting delegated hit testing.</p> <p>There isn’t one single set of abstractions that best solves this whole problem. We can slice up a problem space in multiple ways, and we have to flip between representations depending on what we’re trying to do.</p> <p>In JavaScript a few years ago, when you wanted to generate a list of strings from a list of numbers, you would allocate an empty list for the strings, write a <code class="language-plaintext highlighter-rouge">for</code>-loop to scan the numbers, and format each number into a string before adding it to the string list:</p> <div class="language-js highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">const</span> <span class="nx">n</span> <span class="o">=</span> <span class="nf">getNumbers</span><span class="p">();</span>
<span class="kd">const</span> <span class="nx">s</span> <span class="o">=</span> <span class="p">[];</span>

<span class="k">for </span><span class="p">(</span><span class="kd">let</span> <span class="nx">c</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="nx">c</span> <span class="o">&lt;</span> <span class="nx">n</span><span class="p">.</span><span class="nx">length</span><span class="p">;</span> <span class="nx">c</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
    <span class="nx">s</span><span class="p">.</span><span class="nf">push</span><span class="p">(</span><span class="s2">`Item </span><span class="p">${</span><span class="nx">n</span><span class="p">[</span><span class="nx">c</span><span class="p">]}</span><span class="s2">`</span><span class="p">);</span>
<span class="p">}</span>
</code></pre></div></div> <p>Programs were full of little loops like that. Now we say:</p> <div class="language-js highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">const</span> <span class="nx">s</span> <span class="o">=</span> <span class="nf">getNumbers</span><span class="p">().</span><span class="nf">map</span><span class="p">((</span><span class="nx">n</span><span class="p">)</span> <span class="o">=&gt;</span> <span class="s2">`Item </span><span class="p">${</span><span class="nx">n</span><span class="p">}</span><span class="s2">`</span><span class="p">);</span>
</code></pre></div></div> <p>The <code class="language-plaintext highlighter-rouge">map</code> thing is an abstraction, a black box, a building block. Someone had to invent it, strange as that seems. But now we accept it as fundamental without thinking about it. A <code class="language-plaintext highlighter-rouge">Promise</code> is an abstraction over “perhaps not yet”, which is a super vague ephemeral-sounding thing, but our code is full of them. They have reality and solidity, purely because we need them to.</p> <h2 id="humans-ugh">Humans, Ugh</h2> <p>Yet despite their essential importance in helping us understand both the real and software worlds, abstractions are nevertheless a source of controversy. The reason is obvious: they are things summoned by human beings for our convenience, and we often annoy each other. We’re in the realm of the dreaded <em>social problems</em>:</p> <ul> <li> <p>New senior member joins team and is assigned a grand new goal (how motivational!) But they find the existing code is built on abstractions that just get in their way. In fact they’re pretty sure that if all that abstract nonsense was torn down and replaced with reams of atomic verbosity, it would be easier to build the new abstractions they actually need. It would have been better if the last bunch of bozos hadn’t tried to be so clever and invent so many useless abstractions.</p> </li> <li> <p>New junior member joins team and is given a small, easy goal (how humble!) but they struggle to implement it because they’re in the straightjacket of whatever abstractions already exist. Junior believes too strongly in the existence of abstractions laid down by others. Their existence must be <em>honoured</em>. Junior must complete the goal in a way that pays tribute to The Way Things Are Done.</p> </li> <li> <p>Junior colleague thinks a different abstraction would help clarify the code. Senior colleague is threatened by this because it seems to suggest that a thing exists that senior wasn’t aware of, and senior is supposed to know about things like that, not be taught their A-B-Cs by some jumped-up junior. So senior bullies junior into accepting the non-existence of this new thing, and junior internalizes this abuse.</p> </li> </ul> <p>These are not problems with the practice of abstraction in general. Nor are they necessarily a sign of a problem with any specific abstraction: if you can find a situation where the abstraction is unhelpful, that doesn’t make it a bad abstraction. What about the dozen or so situations where it’s helpful? No abstraction is universally applicable. It could be said that you don’t understand an abstraction until you know the limits of its applicability. Don’t try to use <code class="language-plaintext highlighter-rouge">map</code> when you are not projecting a list of items into another list of items.</p> <p>The problems are of perception, and social obligation, and impatience, and intolerance. Yes, there’s always that one person who quotes chapter and verse instead of thinking. They justify a decision by reciting the SOLID principles, and they may even close their eyes to show that they are performing an incredible feat of recall from memory as they do so. I know, it’s annoying. But are you going to replace <em>all</em> your <code class="language-plaintext highlighter-rouge">map</code>s with <code class="language-plaintext highlighter-rouge">for</code>-loops? Are you really <em>that</em> much of an atheist?</p> <h2 id="opinions">Opinions</h2> <p>This brings me to <a href="https://www.sandimetz.com/blog/2016/1/20/the-wrong-abstraction">a blog post from a few years back</a>, which expands on the advice:</p> <blockquote> <p>prefer duplication over the wrong abstraction</p> </blockquote> <p>Clearly the wrong abstraction is… well, wrong. And it becomes clear that Metz is not talking about abstractions that call out to you and feel like they have independent existence. He’s referring to the situation where we spot two copies of the same code, pull it out into one copy and give it a name. Naming things is hard, so we could make a terrible mistake at this stage, but otherwise it’s little model of the birth of every single abstraction that has ever been created.</p> <p>The real problem is what happens later:</p> <blockquote> <p>Programmer B feels honor-bound to retain the existing abstraction, but since isn’t exactly the same for every case, they alter the code to take a parameter, and then add logic to conditionally do the right thing based on the value of that parameter.</p> </blockquote> <p>Consider <code class="language-plaintext highlighter-rouge">map</code>. Sometimes we want to omit some items from the output list. We could arrange this by having <code class="language-plaintext highlighter-rouge">map</code> discard <code class="language-plaintext highlighter-rouge">null</code>s, so we can write the little lambda to return <code class="language-plaintext highlighter-rouge">null</code> for any items we don’t need. But what about other uses of <code class="language-plaintext highlighter-rouge">map</code> where we need to be able to retain <code class="language-plaintext highlighter-rouge">null</code>s? No problem, just add a new parameter to <code class="language-plaintext highlighter-rouge">map</code>, a boolean flag called <code class="language-plaintext highlighter-rouge">discardNulls</code>. This is easy! Also sometimes we want to product <em>multiple</em> output items from a single input. We could get <code class="language-plaintext highlighter-rouge">map</code> to accept a function that optionally returns arrays of outputs, and have <code class="language-plaintext highlighter-rouge">map</code> flatten all those little arrays into one single output array. But again, what if sometimes we want to keep the individual arrays? No problem, just add a new boolean parameter called <code class="language-plaintext highlighter-rouge">flattenArrays</code>…</p> <p>Fortunately someone already went on this journey for us, so we know the right answer is to leave <code class="language-plaintext highlighter-rouge">map</code> alone and invent <code class="language-plaintext highlighter-rouge">filter</code> and <code class="language-plaintext highlighter-rouge">flatMap</code>. Though actually we could re-write <code class="language-plaintext highlighter-rouge">map</code> to use <code class="language-plaintext highlighter-rouge">flatMap</code>.</p> <div class="language-js highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">map</span><span class="p">(</span><span class="nx">f</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">return</span> <span class="k">this</span><span class="p">.</span><span class="nf">flapMap</span><span class="p">(</span><span class="nx">i</span> <span class="o">=&gt;</span> <span class="p">[</span><span class="nf">f</span><span class="p">(</span><span class="nx">i</span><span class="p">)]);</span><span class="nx">x</span>
<span class="p">}</span>
</code></pre></div></div> <p>Similarly <code class="language-plaintext highlighter-rouge">filter</code> would use <code class="language-plaintext highlighter-rouge">map</code> to produce either a one-item array or an empty array for each input:</p> <div class="language-js highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">filter</span><span class="p">(</span><span class="nx">f</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">return</span> <span class="k">this</span><span class="p">.</span><span class="nf">flapMap</span><span class="p">(</span><span class="nx">i</span> <span class="o">=&gt;</span> <span class="nf">f</span><span class="p">(</span><span class="nx">i</span><span class="p">)</span> <span class="p">?</span> <span class="p">[</span><span class="nx">i</span><span class="p">]</span> <span class="p">:</span> <span class="p">[]);</span>
<span class="p">}</span>
</code></pre></div></div> <p>It’s likely that for performance reasons (and because these operations are so simple and permanent anyway) no JS runtime internally works that way, but nevertheless it’s a microcosmic example of how abstractions can layer on top of each other. It’s also an example of a situation where you may not bother to layer them, and that is also instructive. Sometimes it just ain’t worth it. The result may be shorter, but less easy to understand.</p> <p>Assuming there is some existing abstraction that seems tantalisingly close to what we need, burdening it with new responsibilities is usually the wrong thing to do. On the other hand, <em>removing</em> responsibilities can make it more general, and thus applicable to more situations, and therefore <em>less likely to need changes in future</em>, without making it more complex. That way you avoid those accumulations of bug-prone knotiness.</p> <p>Another <a href="https://overreacted.io/goodbye-clean-code/">more recent blog post</a> by Dan Abramov touched on the same topics. He told quite a sad story of a confrontational moment, and he berates his younger self. I’d say he goes too far in doing so. I’m going to stick up for Abramov the Younger. If you’re writing an editor that shows little handles we can drag to resize objects, then it should not be at all controversial to assert that <em>handles are a thing</em>.</p> <p>It’s possible he did something really bad with this idea. Maybe he imposed a grand structure full of assumptions? From the elided snippets it’s not clear. He says:</p> <blockquote> <p>For example, we later needed many special cases and behaviors for different handles on different shapes. My abstraction would have to become several times more convoluted to afford that, whereas with the original “messy” version such changes stayed easy as cake.</p> </blockquote> <p>So it may be that with his changes, the whole editor became too aware of a specific way of implementing handles in terms of orthogonal edges and their intersections, making it impossible to add some weird new shape (a free-hand polygon?) without first shoehorning it into an inappropriate boxy straightjacket.</p> <p>If the original approach was superior in this regard, it must have been by making fewer assumptions. The snippets don’t suggest this, but I’d guess the editor would need a way of delegating the handling of mouse events down to the shapes, and of delegating the process of painting the UI of the shape. That is, shape types are polymorphic extensions to a system that knows nothing of their internal details, and doesn’t force them to implement handles at all, let alone in a particular way. A shape can basically do whatever it wants. This is a subtle point, but a crucial one. Such an editing system is more general, knows less, and allows a wider range of ideas to live within it and cooperate with each other.</p> <p>That way, you can create a helpful way to implement handles that has applicability to certain limited situations, and the shapes that use it will be easier to understand, because their code will have the same structure as their UI. The very same things we can directly manipulate on the screen will also exist in the code, in an immediately familiar and recognisable way.</p> <p>Of course, this hardly needs explaining to anyone from the React team, authors as they are of a framework predicated on the value of defining your own little vocabulary of nestable abstractions that relate directly to things that appear in the UI. React’s tutorials do not tell you to copy and paste the same mess of JSX elements fifty times, for obvious reasons.</p> <p>Even so, Abramov seems to have taken this one bruising encounter and concluded that “abstractions” themselves are the problem. He derides the inventing of abstractions as a phase we all go through before we grow out of it, a self-deception, almost a psychological condition stemming from youthful insecurity.</p> <p>Was the inventor of <code class="language-plaintext highlighter-rouge">map</code> also driven by an obsessive urge to feel self-worth? Who can say. But we can surely be glad they invented it, and in the end, they did it by spotting some noisy boilerplate code cropping up everywhere and abstracting out a simple, helpful <em>thing</em>.</p> <h2 id="who-are-you-calling-dirty">Who Are You Calling Dirty?</h2> <p>So there is something else going on here, and <a href="https://dev.to/d_ir/clean-code-dirty-code-human-code-6nm">I think this take</a> is on to something: the ridiculous word <em>clean</em>. It’s not explanatory, it’s just a terrible value-laden way to start a conversation with a coworker. “Look, I had to clean up the mess you made” is openly provocative.</p> <p>So it’s possible that when people complain about abstractions, they’re talking about one of two things:</p> <ul> <li>Trying to make something “more reusable” by increasing its complexity, which is sort of the exact opposite of how to do that.</li> <li>Suggesting changes to code written by others in a way that will make them react explosively.</li> </ul> <p>Both of these will lead to bad experiences, but they can be avoided without abandoning the idea of collaborative abstraction-building, which, in the end, is all we are doing. Every software product is an abstraction, composed of abstractions, composed of yet more abstractions. It’s abstractions all the way down, and we have to invent them together as teams, and help each other as best we can.</p>]]></content><author><name></name></author><category term="style"/><summary type="html"><![CDATA[When aliens finally pay us a visit and they start floating around our cities, struggling to pronounce greetings from a phrasebook, we will no doubt say to one another, “Apparently aliens are a thing now.” When we recognise something has started happening all the time, we call it “a thing”. Or we might remind our friend in a tone of heavy irony, after they accidentally walk into a lamppost, “Yup, lampposts are still a thing.”]]></summary></entry><entry><title type="html">Unfortunate Bifurcations</title><link href="https://danielearwicker.github.io/blog/2019/Unfortunate-Bifurcations/" rel="alternate" type="text/html" title="Unfortunate Bifurcations"/><published>2019-11-24T00:00:00+00:00</published><updated>2019-11-24T00:00:00+00:00</updated><id>https://danielearwicker.github.io/blog/2019/Unfortunate-Bifurcations</id><content type="html" xml:base="https://danielearwicker.github.io/blog/2019/Unfortunate-Bifurcations/"><![CDATA[<p>Although this is going to seem like a series of picky complaints about C#, really it’s about how any language has to evolve, and is a compromise between past and future, and the whole thing is quite difficult.</p> <p>Also some speculation on what the future of language interoperability will be.</p> <p>The kind of problem I’m going to pick on is where languages separate two concepts and treat them differently, making a virtue of the differences, but then it becomes a pain dealing with them generically. The language designers seem to be saying “You shouldn’t need to treat these two things the same; they’re fundamentally different. You’re doing it all wrong!” And yet…</p> <p>Java did this with its separation between:</p> <ul> <li>user-defined (and standard library) types, known as classes, whose objects are <em>referred to</em> by variables known as references, and can be aliased (that is, more than one variable can refer to the same object), and have no built-in way of being copied, and (as a technical detail) are garbage collected. Assignment always means “copy the identity so as to refer to the same object”, and equality means “same object?”</li> <li>primitive built-in types such as <code class="language-plaintext highlighter-rouge">boolean</code> and various numeric ones that are always owned by a single variable or field, and can all be copied. They either live on the stack or inside another object. Assignment means “copy the value” and equality means “same value?”</li> </ul> <p>It seems at first that I can use either as the type of a variable, or a parameter, or a returned value, so they’re there are a lot of places where they are interchangeable, but these ultimately fall apart in various annoying ways. I can define my own types for objects that can be aliased, but not if I want to create an object that can’t be aliased. There is further pain when dealing with generics. Only class types can be used as type parameters. Primitives have to be <em>boxed</em> (stored inside a wrapper object of class type), so every primitive has a corresponding box type. This damages performance enough that most general purpose libraries have to provide <a href="https://docs.oracle.com/javase/8/docs/api/java/util/stream/IntStream.html">primitive-specific specialisations of their classes</a>. The infection even <a href="https://twitter.com/kotlin/status/1195295424963235840">spreads into other languages that target the JVM</a>.</p> <p>The language automatically coerces primitives to their box type where it can, but <a href="https://stackoverflow.com/a/5199418/27423">this can lead to strange problems</a> due to the different meaning of equality for class and primitive types, and some unfortunate details of how auto-boxing works.</p> <p>C# improved on this situation a lot. With its <code class="language-plaintext highlighter-rouge">struct</code> keyword it lets you define your own compound types that work just like primitives, and collectively they are all known as <em>value</em> types. Auto-boxing is a lot more seamless. Also you can define what the <code class="language-plaintext highlighter-rouge">==</code> operation means on your types, which can be used to hide the differences. Finally, it did a much better job with generics, eliminating most needs for boxing and hand-maintained specialisations.</p> <p>Generics provide us with a way to be abstract about certain details. Suppose we want to capture the pattern of retrying operations. A basic example:</p> <div class="language-cs highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">var</span> <span class="n">answer</span> <span class="p">=</span> <span class="n">Util</span><span class="p">.</span><span class="nf">Retry</span><span class="p">(()</span> <span class="p">=&gt;</span> <span class="nf">GetTheAnswer</span><span class="p">());</span>
</code></pre></div></div> <p>That <code class="language-plaintext highlighter-rouge">Retry</code> method calls the operation passed into it until it succeeds without throwing, and passed back the result. What type is the parameter to <code class="language-plaintext highlighter-rouge">Retry</code>? It needs to return a value, so it’s going to be <code class="language-plaintext highlighter-rouge">Func&lt;T&gt;</code>. C# will infer the <code class="language-plaintext highlighter-rouge">T</code> from my usage, so <code class="language-plaintext highlighter-rouge">answer</code> ends up having the same type as is returned by <code class="language-plaintext highlighter-rouge">GetTheAnswer</code>. Neat.</p> <p>The efficient way to deal with values of primitive types is likely to be different to that for handling reference types, but this detail is hidden from us in C# - the JIT automatically produces one instance of directly executable machine code to be used for all reference types in place of <code class="language-plaintext highlighter-rouge">T</code>, and then one further instance for every value type we use. This expansion <em>isn’t</em> done by the C# compiler, which just has to produce a single generic version of the <a href="https://en.wikipedia.org/wiki/Common_Intermediate_Language">CIL bytecode</a>.</p> <p>How about:</p> <div class="language-cs highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Util</span><span class="p">.</span><span class="nf">Retry</span><span class="p">(()</span> <span class="p">=&gt;</span> <span class="nf">CauseTheSideEffect</span><span class="p">());</span>
</code></pre></div></div> <p>Same idea, but now I don’t need a return value, because <code class="language-plaintext highlighter-rouge">CauseTheSideEffect</code> “returns” <code class="language-plaintext highlighter-rouge">void</code>. Is this going to work? <code class="language-plaintext highlighter-rouge">Retry</code> is going to be something like this:</p> <div class="language-cs highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">T</span> <span class="n">Retry</span><span class="p">&lt;</span><span class="n">T</span><span class="p">&gt;(</span><span class="n">Func</span><span class="p">&lt;</span><span class="n">T</span><span class="p">&gt;</span> <span class="n">operation</span><span class="p">)</span>
<span class="p">{</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">n</span> <span class="p">=</span> <span class="m">4</span><span class="p">;</span> <span class="n">n</span> <span class="p">&gt;=</span> <span class="m">0</span><span class="p">;</span> <span class="n">n</span><span class="p">--)</span>
    <span class="p">{</span>
        <span class="k">try</span>
        <span class="p">{</span>
            <span class="k">return</span> <span class="nf">operation</span><span class="p">();</span>
        <span class="p">}</span>
        <span class="k">catch</span> <span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">when</span> <span class="p">(</span><span class="n">x</span> <span class="p">&gt;</span> <span class="m">0</span><span class="p">)</span>
        <span class="p">{</span>
            <span class="c1">// log x?</span>
        <span class="p">}</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div> <p>So we’d like <code class="language-plaintext highlighter-rouge">T</code> to be able to be <code class="language-plaintext highlighter-rouge">void</code>. It doesn’t seem to be asking much, because we’re never doing anything with <code class="language-plaintext highlighter-rouge">T</code> as a value; we just return it. It seems like the language could be lax about this and let a <code class="language-plaintext highlighter-rouge">return</code> statement precede a call to a <code class="language-plaintext highlighter-rouge">void</code> method inside another <code class="language-plaintext highlighter-rouge">void</code> method. <a href="https://www.geeksforgeeks.org/return-void-functions-c/">This is what C++ does.</a>.</p> <p>But C++ can get away with this because it instantiates its version of generics (templates) by pretty much replaying your source code like a macro, so if <code class="language-plaintext highlighter-rouge">void</code> caused trouble somewhere inside the template’s code this would produce an error message, often quite confusingly. C# doesn’t work like that - it produces <em>one</em> version of your code in CIL, and CIL uses a different instruction for calling a <code class="language-plaintext highlighter-rouge">void</code> method. This unfortunate bifurcation runs deep.</p> <p>We can mask the problem by providing an overload like this:</p> <div class="language-cs highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">void</span> <span class="nf">Retry</span><span class="p">(</span><span class="n">Action</span> <span class="n">operation</span><span class="p">)</span>
     <span class="p">=&gt;</span> <span class="nf">Retry</span><span class="p">(()</span> <span class="p">=&gt;</span>
        <span class="p">{</span>
            <span class="nf">operation</span><span class="p">();</span>
            <span class="k">return</span> <span class="m">0</span><span class="p">;</span>
        <span class="p">});</span>
</code></pre></div></div> <p>The return value is arbitrary. And so in theory the C# compiler could allow us to use the underlying <code class="language-plaintext highlighter-rouge">Func&lt;T&gt;</code> version of <code class="language-plaintext highlighter-rouge">Retry</code> by noticing we aren’t returning anything and therefore filling in the <code class="language-plaintext highlighter-rouge">return 0;</code> for us at the point of use. But obviously it shouldn’t always do this, because it would weaken the compiler’s ability to spot bugs, due to it silently passing dummy values into our code.</p> <p>There are other bifurcations that are even more problematic. The worst is probably <code class="language-plaintext highlighter-rouge">async</code>/<code class="language-plaintext highlighter-rouge">await</code>. Extending our example:</p> <div class="language-cs highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">var</span> <span class="n">answer</span> <span class="p">=</span> <span class="k">await</span> <span class="n">Util</span><span class="p">.</span><span class="nf">Retry</span><span class="p">(()</span> <span class="p">=&gt;</span> <span class="nf">GetTheAnswerAsync</span><span class="p">());</span>
</code></pre></div></div> <p>Now we have to re-write retry to accept a <code class="language-plaintext highlighter-rouge">Func&lt;Task&lt;T&gt;&gt;</code> and use <code class="language-plaintext highlighter-rouge">async</code>/<code class="language-plaintext highlighter-rouge">await</code> internally, and then restore our original synchronous version via a wrapper overload:</p> <div class="language-cs highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">T</span> <span class="nf">Retry</span><span class="p">(</span><span class="n">Func</span><span class="p">&lt;</span><span class="n">T</span><span class="p">&gt;</span> <span class="n">operation</span><span class="p">)</span>
     <span class="p">=&gt;</span> <span class="nf">Retry</span><span class="p">(()</span> <span class="p">=&gt;</span> <span class="n">Task</span><span class="p">.</span><span class="nf">FromResult</span><span class="p">(</span><span class="nf">operation</span><span class="p">())).</span><span class="n">Result</span><span class="p">;</span>
</code></pre></div></div> <p>So we have to wrap the result of the inner <code class="language-plaintext highlighter-rouge">operation</code> in a <code class="language-plaintext highlighter-rouge">Task&lt;T&gt;</code> and then extract the <code class="language-plaintext highlighter-rouge">Result</code> on the outside. Thanks to the way <code class="language-plaintext highlighter-rouge">await</code> works this won’t actually involve any hidden asynchrony: the inner <code class="language-plaintext highlighter-rouge">Task&lt;T&gt;</code> is already completed, so <code class="language-plaintext highlighter-rouge">await</code> doesn’t try to yield control, and similarly the <code class="language-plaintext highlighter-rouge">Result</code> property doesn’t need to <code class="language-plaintext highlighter-rouge">Wait</code>.</p> <p>It’s once you have two such bifurcations that things like <code class="language-plaintext highlighter-rouge">Retry</code> become tedious: you need <em>four</em> overloads to cover every case.</p> <p>With the addition of nullable references in C# 8 there is another nasty example, which is actually the old split between reference and value types coming back to bite us. Surprisingly, there is currently no way to express <code class="language-plaintext highlighter-rouge">T?</code> where <code class="language-plaintext highlighter-rouge">T</code> is any type, value or reference. Support for nullable value types has been in the language for a very long time, but they work very differently because for a value type adding support for an additional <code class="language-plaintext highlighter-rouge">null</code> state requires extra storage along side the value itself (and to get at the value requires you to look in the <code class="language-plaintext highlighter-rouge">Value</code> property). Reference types by contrast have always supported the special <code class="language-plaintext highlighter-rouge">null</code> value; what’s being added now is the ability to constrain them so they (mostly) don’t allow <code class="language-plaintext highlighter-rouge">null</code>, which is essentially a compile-time concept. So although the language seems to have a general concept of a nullable “thing”, it really doesn’t. It just uses the same <code class="language-plaintext highlighter-rouge">?</code> suffix syntax to denote the nullable variants of two entirely different things.</p> <p>As a deliberately simple example consider the good old <em>Maybe</em> monadic bind operator, popularly defined as <code class="language-plaintext highlighter-rouge">IsNotNull</code>:</p> <div class="language-cs highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">public</span> <span class="k">static</span> <span class="n">TResult</span><span class="p">?</span> <span class="n">IsNotNull</span><span class="p">&lt;</span><span class="n">TArg</span><span class="p">,</span> <span class="n">TResult</span><span class="p">&gt;(</span>
    <span class="k">this</span> <span class="n">TArg</span><span class="p">?</span> <span class="n">arg</span><span class="p">,</span>
    <span class="n">Func</span><span class="p">&lt;</span><span class="n">TArg</span><span class="p">,</span> <span class="n">TResult</span><span class="p">&gt;</span> <span class="n">operation</span><span class="p">)</span>
        <span class="k">where</span> <span class="n">TArg</span> <span class="p">:</span> <span class="k">class</span>
        <span class="nc">where</span> <span class="n">TResult</span> <span class="p">:</span> <span class="k">class</span>
            <span class="err">=&gt;</span> <span class="nc">arg</span> <span class="p">!=</span> <span class="k">null</span> <span class="p">?</span> <span class="nf">operation</span><span class="p">(</span><span class="n">arg</span><span class="p">)</span> <span class="p">:</span> <span class="k">null</span><span class="p">;</span>
</code></pre></div></div> <p>Note that I’ve had to constrain both <code class="language-plaintext highlighter-rouge">TArg</code> and <code class="language-plaintext highlighter-rouge">TResult</code> as being reference types (<code class="language-plaintext highlighter-rouge">where</code>… <code class="language-plaintext highlighter-rouge">class</code>). So to cover every possible combination of <code class="language-plaintext highlighter-rouge">struct</code> and <code class="language-plaintext highlighter-rouge">class</code> for the input and result, I need four overloads! But even worse, this time I can’t cheat by making three of them into simple wrappers that call into a single implementation. A nullable reference type is really just a plain reference type in a compile-time disguise, where as a nullable value type is entirely different from its underlying value type at runtime. We have no choice but to copy and paste the code of our method four times, and make slight modifications to each case:</p> <div class="language-cs highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">public</span> <span class="k">static</span> <span class="n">TResult</span><span class="p">?</span> <span class="n">IsNotNull</span><span class="p">&lt;</span><span class="n">TArg</span><span class="p">,</span> <span class="n">TResult</span><span class="p">&gt;(</span>
    <span class="k">this</span> <span class="n">TArg</span><span class="p">?</span> <span class="n">arg</span><span class="p">,</span>
    <span class="n">Func</span><span class="p">&lt;</span><span class="n">TArg</span><span class="p">,</span> <span class="n">TResult</span><span class="p">?&gt;</span> <span class="n">operation</span><span class="p">)</span>
        <span class="k">where</span> <span class="n">TArg</span> <span class="p">:</span> <span class="k">class</span>
        <span class="nc">where</span> <span class="n">TResult</span> <span class="p">:</span> <span class="k">class</span>
            <span class="err">=&gt;</span> <span class="nc">arg</span> <span class="p">!=</span> <span class="k">null</span> <span class="p">?</span> <span class="nf">operation</span><span class="p">(</span><span class="n">arg</span><span class="p">)</span> <span class="p">:</span> <span class="k">null</span><span class="p">;</span>

<span class="k">public</span> <span class="k">static</span> <span class="n">TResult</span><span class="p">?</span> <span class="n">IsNotNull</span><span class="p">&lt;</span><span class="n">TArg</span><span class="p">,</span> <span class="n">TResult</span><span class="p">&gt;(</span>
    <span class="k">this</span> <span class="n">TArg</span><span class="p">?</span> <span class="n">arg</span><span class="p">,</span>
    <span class="n">Func</span><span class="p">&lt;</span><span class="n">TArg</span><span class="p">,</span> <span class="n">TResult</span><span class="p">?&gt;</span> <span class="n">operation</span><span class="p">)</span>
        <span class="k">where</span> <span class="n">TArg</span> <span class="p">:</span> <span class="k">struct</span>
        <span class="nc">where</span> <span class="n">TResult</span> <span class="p">:</span> <span class="k">class</span>
            <span class="err">=&gt;</span> <span class="nc">arg</span> <span class="p">!=</span> <span class="k">null</span> <span class="p">?</span> <span class="nf">operation</span><span class="p">(</span><span class="n">arg</span><span class="p">.</span><span class="n">Value</span><span class="p">)</span> <span class="p">:</span> <span class="k">null</span><span class="p">;</span>

<span class="k">public</span> <span class="k">static</span> <span class="n">TResult</span><span class="p">?</span> <span class="n">IsNotNull</span><span class="p">&lt;</span><span class="n">TArg</span><span class="p">,</span> <span class="n">TResult</span><span class="p">&gt;(</span>
    <span class="k">this</span> <span class="n">TArg</span><span class="p">?</span> <span class="n">arg</span><span class="p">,</span>
    <span class="n">Func</span><span class="p">&lt;</span><span class="n">TArg</span><span class="p">,</span> <span class="n">TResult</span><span class="p">?&gt;</span> <span class="n">operation</span><span class="p">)</span>
        <span class="k">where</span> <span class="n">TArg</span> <span class="p">:</span> <span class="k">class</span>
        <span class="nc">where</span> <span class="n">TResult</span> <span class="p">:</span> <span class="k">struct</span>
            <span class="err">=&gt;</span> <span class="nc">arg</span> <span class="p">!=</span> <span class="k">null</span> <span class="p">?</span> <span class="nf">operation</span><span class="p">(</span><span class="n">arg</span><span class="p">)</span> <span class="p">:</span> <span class="k">default</span><span class="p">;</span>

<span class="k">public</span> <span class="k">static</span> <span class="n">TResult</span><span class="p">?</span> <span class="n">IsNotNull</span><span class="p">&lt;</span><span class="n">TArg</span><span class="p">,</span> <span class="n">TResult</span><span class="p">&gt;(</span>
    <span class="k">this</span> <span class="n">TArg</span><span class="p">?</span> <span class="n">arg</span><span class="p">,</span>
    <span class="n">Func</span><span class="p">&lt;</span><span class="n">TArg</span><span class="p">,</span> <span class="n">TResult</span><span class="p">?&gt;</span> <span class="n">operation</span><span class="p">)</span>
        <span class="k">where</span> <span class="n">TArg</span> <span class="p">:</span> <span class="k">struct</span>
        <span class="nc">where</span> <span class="n">TResult</span> <span class="p">:</span> <span class="k">struct</span>
            <span class="err">=&gt;</span> <span class="nc">arg</span> <span class="p">!=</span> <span class="k">null</span> <span class="p">?</span> <span class="nf">operation</span><span class="p">(</span><span class="n">arg</span><span class="p">.</span><span class="n">Value</span><span class="p">)</span> <span class="p">:</span> <span class="k">default</span><span class="p">;</span>
</code></pre></div></div> <p>When we say <code class="language-plaintext highlighter-rouge">arg != null</code>, in half the cases (where <code class="language-plaintext highlighter-rouge">arg</code> is a value type) that’s just sugar for <code class="language-plaintext highlighter-rouge">arg.HasValue</code>, but such sugar is non-existent when we need to get the value: we have to say <code class="language-plaintext highlighter-rouge">arg.Value</code>. Also when we want to substitute <code class="language-plaintext highlighter-rouge">null</code>, in half the cases (where the result is a value type) we have to use the <code class="language-plaintext highlighter-rouge">default</code> keyword, which is the 7.x abbreviation of <code class="language-plaintext highlighter-rouge">default(TResult?)</code> and means “<code class="language-plaintext highlighter-rouge">Nullable&lt;TResult&gt;</code> with no value”.</p> <p>If this was a less trivial example, it would be a genuine pain to maintain those four version. If you had to add another generic nullable parameter, it would double again the number of hand-maintained not-quite-the-same overloads required.</p> <p>Now combine that with <code class="language-plaintext highlighter-rouge">async</code> versions of everything and you double the overloads again. See how these bifurcations get out hand - before you know it <a href="https://en.wikipedia.org/wiki/Wheat_and_chessboard_problem#Second_half_of_the_chessboard">you’re in the second half of the chessboard.</a> Okay, that’s a slight exaggeration.</p> <p>Anyway, this kind of evolutionary pain is why people start again with new languages. But I think the way forward is already indicated. The CLR is a runtime that is too opinionated and richly featured. This was intended to create a way forward so that a wide range of languages could share libraries with each other. When that happens, it will be utopia compared with today.</p> <p>But the CLR isn’t going to be the platform for that utopia. It was intended to be general enough to support all languages, but now even its flagship showcase language, C#, is showing the strain of supporting its real life users while constrained by the CLR’s underlying model. Yes, it’s better than the JVM, but that’s a very low bar.</p> <p>Meanwhile the last decade has seen runtimes for Javascript become so ingeniously self-optimising that they can compete with native code even for raw number crunching. There are 3D game engines written in JS. There are emulators for mainstream processors written in plain JS that <a href="https://bellard.org/jslinux/">can boot actual operating systems in the browser</a> - nearly a decade ago this was done for a minimal Linux, but Windows 2000 is now there too.</p> <p>WebAssembly in a sense grew out of such efforts, but it has only just started. At the moment it provides a sandbox within which an old-school native C/C++ codebase can freely scribble over its own patch of memory without causing wider damage. It does not yet define how a hosted language may expose fine grained objects that will be automatically garbage collected, and can have named members inside them, some of which may be callable. Hopefully when that step is taken, it will initially be as minimal and vague as possible, instead of (as the CLR did) trying to cover every possible approach with fine-grained features.</p> <p>And then we will have a real breakthrough, because a wide range of languages will be able to move on to the super-fast JS runtimes and bring their libraries with them. We will be able to create data structures that lace together objects written in C#, JavaScript and Python, all to be collected by the same GC.</p> <p>TypeScript has shown that a type system can be organically fitted over a very dynamic object model, and it can grow to meet user needs in ways that long ago left C# in the dust. I wonder if the future of C# lies in switching its home runtime from the CLR to JS.</p>]]></content><author><name></name></author><category term="C#"/><summary type="html"><![CDATA[Although this is going to seem like a series of picky complaints about C#, really it’s about how any language has to evolve, and is a compromise between past and future, and the whole thing is quite difficult.]]></summary></entry><entry><title type="html">Two Cheers for SQL</title><link href="https://danielearwicker.github.io/blog/2019/SQL/" rel="alternate" type="text/html" title="Two Cheers for SQL"/><published>2019-08-26T00:00:00+00:00</published><updated>2019-08-26T00:00:00+00:00</updated><id>https://danielearwicker.github.io/blog/2019/SQL</id><content type="html" xml:base="https://danielearwicker.github.io/blog/2019/SQL/"><![CDATA[<p>What is there to say about this old stain on the technology landscape? Settle in…</p> <p>SQL is not “cool”. It probably never has been. On the one hand there are the technologies we hate, and on the other the technologies no one uses.</p> <p>Having spent a few years going back and forth on the merits of SQL, I’m in a weird place. I now think it is both underrated <em>and</em> overrated, and not merely because other people are too extreme in their opinions. I genuinely think SQL is both a fine idea and a terrible idea at the same time. There is a way of using it that makes sense, and many other ways that don’t.</p> <h2 id="is-select-a-satire-on-orthogonality">Is <code class="language-plaintext highlighter-rouge">select</code> a satire on orthogonality?</h2> <p>The heart of SQL is <code class="language-plaintext highlighter-rouge">select</code>. What’s wrong with it?</p> <p>All modern (“cool”) language features for querying data take a consistent pipeline approach, letting you perform successive transformations on data sets, chaining them together in a way that feels natural, like constructive exploration. But not SQL.</p> <p>The columns in the output of your query are what you have to write down <em>first</em>. This is the opposite of the natural way to think creatively about data. It also breaks apart the two dimensions of the output:</p> <ul> <li>What columns should be included?</li> <li>What rows should be formed?</li> </ul> <p>The columns have to be stated at the start, the rows are (usually) formed at the end. It also confounds any possibility of your editor making helpful auto-completion suggestions. I generally find myself writing <code class="language-plaintext highlighter-rouge">select * from</code> first so I can start from the actual beginning, the source tables, and then come back and replace <code class="language-plaintext highlighter-rouge">*</code> later when I’ve figured out how to get the data I need with the help of the editor’s UI.</p> <p>The above flaw is corrected by all modern frameworks that provide a consistent way of piping data through composable operators, because they naturally start with the input and let you evolve it, one stage at a time, switching between operators as you require.</p> <p>And really this is just one symptom of the broader problem with <code class="language-plaintext highlighter-rouge">select</code>, which is that there’s nothing modular or systematic about the various features that are crammed into it. They don’t compose in flexible ways. They compose in exactly one pre-determined way: <code class="language-plaintext highlighter-rouge">select</code> columns, <code class="language-plaintext highlighter-rouge">from</code> a table, then optionally add <code class="language-plaintext highlighter-rouge">join</code>s to other tables (or <code class="language-plaintext highlighter-rouge">apply</code>, and how about <code class="language-plaintext highlighter-rouge">pivot</code> or <code class="language-plaintext highlighter-rouge">unpivot</code>?), then optionally a <code class="language-plaintext highlighter-rouge">where</code>, then optionally a <code class="language-plaintext highlighter-rouge">group by</code>, then optionally a <code class="language-plaintext highlighter-rouge">having</code>, then optionally an <code class="language-plaintext highlighter-rouge">order by</code>.</p> <p>It’s less like artfully constructing from separate building blocks, and more like filling out a bureaucratic paper form, leaving the irrelevant items blank. Ideas have been retconned into it over the decades, and special homes had to be found for them, because the underlying model is not extensible. In any given revision, it has always carried the pretension that it is finished now, rather than the humility to know that it is never finished and must be open to extension.</p> <h2 id="how-to-do-composition">How to do composition</h2> <p>But SQL does have a general way of composing operations into a pipeline: <strong>common table expressions</strong>. It’s been in the ANSI standard for 20 years. It’s often thought of just as a way to achieve recursion in queries, but it can also be used simply as way of structuring complex queries so they remain modular and readable. If you restrict yourself to only using one feature of <code class="language-plaintext highlighter-rouge">select</code> in each stage, you can get close to the ideal.</p> <p>Here I want to discover patterns in monetary transactions (going into or out of a business) with different vendors. Each stage can see all the previous stages as if they were views or tables.</p> <div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">-- only interested in recent transactions</span>
<span class="k">with</span> <span class="n">recent_trans</span> <span class="k">as</span> <span class="p">(</span>

    <span class="k">select</span> <span class="n">vendor_id</span><span class="p">,</span>
           <span class="k">left</span><span class="p">(</span><span class="n">amount</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="k">as</span> <span class="n">first_digit</span><span class="p">,</span>
           <span class="n">amount</span>
           <span class="c1">-- other columns...</span>
    <span class="k">from</span> <span class="n">trans</span>
    <span class="k">where</span> <span class="n">entered_date</span> <span class="o">&gt;=</span> <span class="o">@</span><span class="n">start_date</span>
<span class="p">),</span>
<span class="c1">-- often only interested in money paid out</span>
<span class="n">invoices</span> <span class="k">as</span> <span class="p">(</span>

    <span class="k">select</span> <span class="n">vendor_id</span><span class="p">,</span>
           <span class="n">first_digit</span><span class="p">,</span>
           <span class="n">amount</span>
           <span class="c1">-- other columns...</span>
    <span class="k">from</span> <span class="n">recent_trans</span>
    <span class="k">where</span> <span class="n">amount</span> <span class="o">&gt;=</span> <span class="mi">0</span>

<span class="p">),</span>
<span class="c1">-- how often do specific digits occur first</span>
<span class="n">invoice_first_digit_frequency_by_vendor</span> <span class="k">as</span> <span class="p">(</span>

    <span class="k">select</span> <span class="n">vendor_id</span><span class="p">,</span>
           <span class="n">first_digit</span><span class="p">,</span>
           <span class="k">count</span><span class="p">(</span><span class="o">*</span><span class="p">)</span> <span class="k">as</span> <span class="n">frequency</span>
    <span class="k">from</span> <span class="n">invoices</span>
    <span class="k">group</span> <span class="k">by</span> <span class="n">vendor_id</span><span class="p">,</span>
             <span class="n">first_digit</span>
<span class="p">),</span>
<span class="c1">-- how often do certain amounts occur</span>
<span class="n">invoice_amount_frequency_by_vendor</span> <span class="k">as</span> <span class="p">(</span>

    <span class="k">select</span> <span class="n">vendor_id</span><span class="p">,</span>
           <span class="n">amount</span><span class="p">,</span>
           <span class="k">count</span><span class="p">(</span><span class="o">*</span><span class="p">)</span> <span class="k">as</span> <span class="n">frequency</span>
    <span class="k">from</span> <span class="n">invoices</span>
    <span class="k">group</span> <span class="k">by</span> <span class="n">vendor_id</span><span class="p">,</span> <span class="n">amount</span>
<span class="p">),</span>
<span class="c1">-- and so on...</span>
</code></pre></div></div> <p>At the end I can write an <code class="language-plaintext highlighter-rouge">update</code> that saves all the facts and figures about each vendor into its own row. I give the whole task to SQL and let it figure out the fast way to execute it. This is called a <a href="https://www.sqlshack.com/introduction-set-based-vs-procedural-programming-approaches-t-sql/">set-based operation</a>.</p> <p>With this ability to work in stages that build on previous ones, you can very easily come up with queries of great power and complexity, while still understanding what you’ve done and leaving the result easy for others to understand, maintain and reuse; this is nothing that should surprise anyone with experience of compositional features in any other language. They’re great.</p> <p>But the situation is somewhat misleading in the case of SQL.</p> <h2 id="the-significance-of-performance">The significance of performance</h2> <p>There are many constraints on software: the best advice you’ll get is to place the most priority on making your software easy to modify without breaking it. Modularity helps, information hiding helps (not letting modules know too much about each other), any sort of high level declarative approach will also help.</p> <p>At some point you will have to worry about how efficiently the software does its job; but this is just one of the many constraints, and should not be allowed to wreck the primary concern: ease of modification. It is only through ease of modification that you will be able to make the changes that will make it all go faster.</p> <p><em>“First make it correct, then make it fast.”</em> Sure, but this adage misses a major problem. Some implementations are so slow that they fail to return the correct answer before it becomes worthless. To be that slow <em>is</em> to be incorrect. The most embarrassing bug is a hang; a sufficiently slow implementation is indistinguishable from a hang.</p> <p>This doesn’t mean that performance trumps ease of modification as a concern. But it does mean that we have to understand how our high level description is turned into an executing implementation in order to meet the basic requirement of correctness.</p> <p>In this regard, SQL is not fundamentally different from any other high-level language. But in practice, due to the degree of freedom it exercises, the results can vary so wildly that SQL ends up being very different from regular languages.</p> <p>Any modern high-level language will apply some optimisations to the code you write: it looks for patterns that it can rearrange to make things run faster. These can be “non-local” patterns that involve relationships between lines of code that are far apart in your source and so are not immediately obvious unless you know how the compiler works. But this effect is usually quite limited and predictable; it’s more like a sprinkling of magic that gives you a general speed-up.</p> <p>In a wide range of software scenarios it is an excellent idea to let the accumulated expertise of smart people work on your behalf by getting software to automatically translate your high-level constraints into an optimal low-level form that can be directly executed. That is precisely what happens with a regex: it gets converted into a state machine. Same for any high-level programming language that compiles down to either machine code or something JIT-able.</p> <h2 id="sqls-approach-to-optimisation">SQL’s approach to optimisation</h2> <p>When you send a SQL query to an RDBMS, it also performs a step akin to compilation: the query is analysed to produce an optimised plan that can be executed. But unlike imperative code, there is no structure, provided by you, for it to use as a starting point. The optimisation process starts from scratch, and the approach it takes to constructing the plan is very much dependent on things that aren’t in your query:</p> <ul> <li>the design of the optimiser (which may be closed source)</li> <li>the indexes that exist (which you choose separately, and may even be accumulated automatically by the RDBMS)</li> <li>the known statistics maintained by the RDBMS about the current contents of indexes (statistics which change over time, and therefore may be inaccurate)</li> </ul> <p>Taking my example above that used chained <code class="language-plaintext highlighter-rouge">with... as</code> stages, it is not the case that each stage is evaluated separately and its results stored temporarily to supply to the next stage. If in the full example the only stage that used <code class="language-plaintext highlighter-rouge">recent_trans</code> was <code class="language-plaintext highlighter-rouge">invoices</code> (supposing I only separated them for clarity) then they could be flattened into one stage. The number of stages in my SQL source code does not translate into more stages in the execution plan. To reiterate: the query optimiser rips up your code and starts again. This is the purported advantage of high-level declarations: they define the constraints on what the results should contain, without specifying how those results should be obtained.</p> <p>The theory is that by applying its own understanding of how to run fast, the RDBMS is best placed to interpret your constraints into imperative computational steps are actually executed.</p> <h2 id="building-a-ship-in-a-bottle">Building a ship in a bottle</h2> <p>Have you ever tried to make a ship in a bottle? I haven’t but I understand that you typically build the ship outside the bottle but with the masts folded down so it just barely fits through the bottleneck, and then you pull on little cords to bring the masts upright, leaving people astonished and speculating as to whether you had to hold two pairs of long tweezers through the bottleneck and struggle for years to manipulate each piece into place.</p> <p>Imagine if you <em>did</em> have to build the ship inside the bottle like that? How would that even work? It would be absurdly frustrating.</p> <p>Well, that’s how you optimize a SQL query execution plan. If the plan is doing something stupid, you can’t just edit the plan. The plan is not yours to edit. You have to change the conditions that caused the query planner to choose a bad plan, such that next time it will choose a better one. You are “outside the bottle”, as it were, in the high-level concepts, and the query planner is hidden inside the bottle, messing with the ship. You are very much at its mercy.</p> <p>This metaphor doesn’t quite capture all the maddening aspects of the situation, because at least with tweezers and a clear glass bottle you can see what you’re doing wrong. You have that instant feedback cycle. But in SQL you don’t have a complete specification of how the query planner decides what to do based on the available indexes. It’s as if the bottle is opaque, and you’re holding tweezers equipped with AI. They are trying to guess what you intend as you move them around by the handle, and they keep getting it wrong.</p> <p>By “wrong” of course I just mean “slow”, but as we noted above, such a degree of unpredictability is really no better than simply failing to work. You can think you’ve written a query that performs acceptably, ship it, and only then discover that under some conditions it performs so badly that it is effectively broken. This is where the sales pitch of SQL leaves us disappointed.</p> <p>Of course, the database vendors have the solution: they provide a way for you to see the execution plan - to look inside the bottle, so you can tweak the constraints of your environment until the execution plan doesn’t do something silly.</p> <p>But now we’ve arrived at an absurd place: we’re in the business of modifying a high-level description so that an algorithm we are unaware of will produce the low-level description we require. This means we have to understand both, while never entirely understanding how one produces the other. Small changes made to the high-level description may have huge effects on the low-level.</p> <p>This activity makes no sense. Unless we’re doing this because we enjoy being driven to distraction, we have to take direct control of the low-level description. This means we have to re-write our high-level, nicely composed set-based query in another form.</p> <p>There are ways to do this in SQL. One frequently used approach with a complex query is to make each step dump its output into a temporary table (a non-standard feature that major RDBMS vendors have all implemented in different ways). By doing this, you are effectively controlling the top-level structure of the execution plan, blocking the optimiser from having any influence over it. If you keep the individual steps small and simple then you can pretty much guarantee there is only one, sensible way for the optimiser to execute them.</p> <p>This is not supposed to be necessary, and that’s why ANSI SQL doesn’t include temporary tables. But it is necessary, and that’s why all major RDBMS vendors have added temporary tables.</p> <p>See what I mean? You can get SQL to do what you want, as long as you don’t do what you’re supposed to.</p> <h2 id="acid-hallucinations">ACID hallucinations</h2> <p>In SQL you are encouraged to enforce consistency inside the database. Per SQL, the right way to design systems is to make them brutally fussy, and to refuse to accept <em>wrong</em> inputs, failing with a hard error. If the caller is using the database incorrectly, that’s their problem. The invariants must be preserved. This idea is attractive because if you can reduce the possible states your database might be in, you don’t have to write so much code (and so many tests) to deal with all those impossible states.</p> <p>The range of constraints you can express at the DB level is quite limited: column values can be required to be <code class="language-plaintext highlighter-rouge">unique</code>, or a <code class="language-plaintext highlighter-rouge">foreign key</code>, or meet some simple <code class="language-plaintext highlighter-rouge">check</code>. But this is usually quite enough to tie your database down to an extremely inflexible usage pattern.</p> <p>The problem is, in reality, we often don’t know in advance exactly what these constraints ought to be. Theoretically a <code class="language-plaintext highlighter-rouge">Payment</code> is made to a <code class="language-plaintext highlighter-rouge">Vendor</code>, and so the vendor must exist in the DB before we can store the payment to refer to it. Except…</p> <p>Sometimes the information arrives in the wrong order, so we need to store the payment now, even though we don’t yet have the details for the vendor. In the meantime we could easily tolerate the existence of such orphaned payments; they shouldn’t cause errors. They can just be ignored until the vendor record arrives later. Surprising as it is to some, there’s no absolute rule saying we must have <code class="language-plaintext highlighter-rouge">foreign key</code> constraints for every relationship. Sometimes it would create an entirely artificial and unnecessary limitation.</p> <p>To work flexibly the relationship between the two record types will have to be expressed by a natural key: some external real-world identifier for the vendor. (How often do systems built on SQL make up surrogate IDs in tables that don’t need them and thus make things worse? Probably a lot.) Also by querying for payments to vendors using an <code class="language-plaintext highlighter-rouge">inner join</code> it’s very easy to make incomplete data stay out of sight of your code. Nevertheless, this more ad hoc approach to consistency is often overlooked on the assumption that the “proper” way to do SQL is to plaster your schema with rigid constraints. I’d argue that the proper approach is to solve the actual problem at hand, rather than solving some other, only superficially similar, problem.</p> <p>I’ve worked with teams that have gone for years putting up with the inflexibility of their design because it had foreign key constraints that enforced a particular order to the insertion of data, and they believed (through a kind of psychological conditioning) that if they removed the constraints then something bad would happen. In reality, nothing bad would happen and their design would start behaving in a way that would be immediately better for their users.</p> <p>By the way, SQL’s built-in ACID guarantees are another example of how an abstraction can fall apart when faced with reality. Ultimately the RDBMs has to decide when to lock records, or even whole tables, so that other concurrent queries wait their turn. It does this for you, intelligently. Does this ring a bell? The marketing would suggest that this is another one of those technical details best left to the clever machine to figure out for you from your high-level constraints. You can guess how it works in practice: the locking approach is different between RDBMS vendors, and they give you ways of controlling it that are not covered by the standard.</p> <h2 id="code-generation-aka-build-your-own-high-level-language">Code generation, a.k.a. build your own high-level language</h2> <p>An important technique in creating maintainable systems is code generation, that is, writing your own tools to generate all the repetitive garbage code so you don’t have to hand-maintain it. For me a major sign of the limitations of a language is the frequency with which code generation turns out to be useful. It’s occasionally useful with any language, however well-equipped.</p> <p>But if you find it seems to always be useful with your language, then that suggests that the language is painfully lacking in vital features. I seem to find code generation a continual necessity in SQL. Stack Overflow is full of suggestions involving <code class="language-plaintext highlighter-rouge">exec</code> (SQL Server) or <code class="language-plaintext highlighter-rouge">prepare</code>/<code class="language-plaintext highlighter-rouge">execute</code> (MySQL) or <code class="language-plaintext highlighter-rouge">execute immediate</code> (Oracle) as workarounds. If I want to do something very similar with each of a dozen columns, or I want to do almost the same thing in five different situations except with minor differences, there is generally no way to use a native SQL looping structure or parameterise my code, except by generating the SQL externally.</p> <p>I’m not sure it could get more ironic than a language adhering to a philosophy of providing purely high-level declarative features so you don’t have to write ugly low-level imperative code, but which:</p> <ol> <li> <p>does such a bad job of the translation that you have to restrict your use of it to the point where you are effectively writing low-level imperative code, and</p> </li> <li> <p>provides such a weak set of high-level declarative features that you have to use it as a kind of compilation target for your own language.</p> </li> </ol> <h2 id="whats-actually-going-on-here">What’s actually going on here?</h2> <p>Under the hood of a database, we find standard data structures, and the most important is the <a href="https://en.wikipedia.org/wiki/B-tree">B-Tree</a>, which is an excellent way of sorting large quantities of data by some key, and efficiently distributing it across separate pages that can be read/written as contiguous chunks from the underlying file. It makes it fast to seek to a particular key, and then to scan through adjacent keys in their sorted order. Ultimately the purpose of a SQL database is to make the low-level power of a B-Tree available to you in a convenient high-level form. The primary key of a table serves as the sort key for a B-Tree that holds all that table’s records. Additional indexes on the table’s columns are just further B-Trees that map to the primary key of the target record.</p> <p>How do B-Trees make queries fast? Mainly by keeping data with logically adjacent keys close together. On a very large dataset the slowest thing about querying is reading the data from disk. To seek to a random location averages about 10 ms, which is long enough to read a megabyte sequentially. Seeking is obviously a huge time suck. It could be reduced by keeping data physically sorted on disk, but that would make insertion horribly expensive (imagine having to copy gigabytes of data along by some offset to make room for a new row). A B-Tree compromises by bunching data together into pages, and each page is read as a whole unit. So there is only a need for an expensive seek each time you scan off the end of the current page. The data doesn’t have to be precisely sorted <em>within</em> the pages, but it must be sorted <em>between</em> the pages (that is, it must be partitioned).</p> <p>This understanding is vital to obtaining acceptable, predictable performance. So what does SQL do? It hides these truths from you as much as possible. They are dirty, low-level thoughts and are beneath you. Roll up, one and all, come and play performance roulette!</p> <p>But by defining indexes, and only letting the SQL optimiser loose on very simple queries, you can effectively use an RDBMS as a raw B-Tree engine; a means of storing and caching partitioned data so you can choose how to perform lookups and scans within it.</p> <p>On the other hand, you may find yourself fighting against it more often than you are supported by it. The seductive possibilities of complex queries are always dragging you towards the treacherous cliff-edge of unpredictable performance.</p> <p>So this is why people sometimes invent their own database system from scratch.</p>]]></content><author><name></name></author><category term="sql"/><summary type="html"><![CDATA[What is there to say about this old stain on the technology landscape? Settle in…]]></summary></entry></feed>